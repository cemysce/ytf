#!/usr/bin/env python3

# Copyright (c) 2022, cemysce
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

# YouTube Follow

import abc
import collections
import copy
import datetime
import html
import inspect
import io
import json
import logging
import os
import pathlib
import platform
import subprocess
import sys
import time
import webbrowser

assert sys.version_info >= (3, 8)

USAGE = '''ytf - YouTube Follow

Usage: {program} [generate|view]

  generate  Generates/regenerates report HTML file of new videos on followed
             playlists (logs to log file).

  view      Opens report HTML file in default web browser (logs to console).

Both modes lock report file to prevent concurrent use by other {program} instances.
'''

LOG_FILE_NAME    = 'ytf.log'
CONFIG_FILE_NAME = 'ytf-config.json'
STATE_FILE_NAME  = 'ytf-state.json'
REPORT_FILE_NAME = 'new-videos.html'

class PausedLogFile:
    def __init__(self, logger=None, *, reopen_mode='a'):
        self.__logger      = logger if logger else logging.getLogger()
        self.__reopen_mode = reopen_mode
        self.__fh_props    = None
        self.__fh_removed  = False
    @classmethod
    def __get_logger_with_filehandler(cls, logger):
        fhs = [h for h in logger.handlers
               if isinstance(h, logging.FileHandler)]
        if len(fhs) == 0:
            if logger.propagate and logger.parent:
                logger_with_fh, fh = cls.__get_logger_with_filehandler(logger.parent)
                if fh:
                    return logger_with_fh, fh
            return logger, None
        if len(fhs) > 1:
            raise RuntimeError(f'Logger "{logger.name}" has >1 FileHandler ({len(fhs)}).')
        assert len(fhs) == 1
        return logger, fhs[0]
    def __enter__(self):
        self.__logger, fh = self.__get_logger_with_filehandler(self.__logger)
        if not fh:
            return None
        self.__fh_props = {'fullpath':  fh.baseFilename,
                           'level':     fh.level,
                           'formatter': fh.formatter,
                           'addl_args': {'encoding': fh.encoding}}
        if hasattr(fh, 'errors'):
            # FileHandler.errors was only added in Python 3.9
            self.__fh_props['addl_args']['errors'] = fh.errors
        self.__logger.removeHandler(fh)
        self.__fh_removed = True
        fh.close()
        return self.__fh_props['fullpath']
    def __exit__(self, *exc_details):
        if self.__fh_removed:
            fh = logging.FileHandler(self.__fh_props['fullpath'],
                                     mode=self.__reopen_mode,
                                     **self.__fh_props['addl_args'])
            fh.setLevel(self.__fh_props['level'])
            fh.setFormatter(self.__fh_props['formatter'])
            self.__logger = logging.getLogger()
            self.__logger.addHandler(fh)
        return False

class LoggerMixin:
    def __init__(self, instance_identifying_text=None):
        self.__instance_identifying_text = instance_identifying_text
    def _logger(self):
        current_subclass_name = self.__class__.__name__
        current_frame = inspect.currentframe()
        calling_frame = None
        prefix_str = None
        try:
            calling_frame = current_frame.f_back
            calling_class_name = calling_frame.f_locals['self'].__class__.__name__
            assert current_subclass_name == calling_class_name
            calling_function_name = calling_frame.f_code.co_name
            instance_prefix = (f'{{{self.__instance_identifying_text}}}'
                               if self.__instance_identifying_text else
                               '')
            return logging.getLogger(f'{calling_class_name}{instance_prefix}'
                                     f'.{calling_function_name}')
        except:
            pass
        finally:
            # prevent keeping references to frame objects, since that can cause
            #  program to create reference cycles (see `inspect` module's
            #  documentation)
            del calling_frame
            del current_frame

## GENERIC SAFE FILE-HANDLING UTILITY FUNCTIONS: ##############################

# Python Standard Library's `os.rename` won't clobber on Windows.
def __get_rename_noreplace_for_Windows():
    logger = logging.getLogger('get_rename_noreplace_for_Windows')
    logger.info('Initializing no-replace rename for Windows.')
    # nothing to initialize...
    logger.info('No-replace rename implemented as: os.rename')
    return os.rename

# Linux's system call `renameat2(..., RENAME_NOREPLACE)` won't clobber and is
#  atomic.
def __get_rename_noreplace_for_Linux():
    logger = logging.getLogger('get_rename_noreplace_for_Linux')
    logger.info('Initializing no-replace rename for Linux.')
    import ctypes
    import ctypes.util
    import errno
    libc_name = ctypes.util.find_library('c')
    if libc_name is None:
        raise RuntimeError('Linux libc not found.')
    libc = ctypes.CDLL(libc_name, use_errno=True)
    if not hasattr(libc, 'renameat2'):
        raise RuntimeError(f'Linux libc "{libc_name}" is missing'
                            ' renameat2() system call.')
    AT_FDCWD = -100
    RENAME_NOREPLACE = 1
    def renameat2_noreplace_closure(src, dest):
        rc = libc.renameat2(AT_FDCWD, src.encode('UTF-8'),
                            AT_FDCWD, dest.encode('UTF-8'),
                            RENAME_NOREPLACE)
        if rc == 0:
            return
        if rc == -1:
            _errno = ctypes.get_errno()
            # Raising `OSError` seems to actually raise the appropriate
            #  exception type according to the passed-in `errno`.  Of concern
            #  to us is `EEXIST`, for which it will raise `FileExistsError`.
            raise OSError(_errno, os.strerror(_errno), src, None, dest)
        raise RuntimeError(f'Unexpected rcode {rc} from renameat2() for'
                           f' renaming "{src}" to "{dest}".')
    logger.info('No-replace rename implemented as:'
                ' Linux syscall renameat2(RENAME_NOREPLACE)')
    return renameat2_noreplace_closure

# macOS's system call `renamex_np(..., RENAME_EXCL)` won't clobber and is
#  atomic.
def __get_rename_noreplace_for_macOS():
    logger = logging.getLogger('get_rename_noreplace_for_macOS')
    logger.info('Initializing no-replace rename for macOS.')
    import ctypes
    import ctypes.util
    import errno
    libc_name = ctypes.util.find_library('c')
    if libc_name is None:
        raise RuntimeError('macOS libc not found.')
    libc = ctypes.CDLL(libc_name, use_errno=True)
    if not hasattr(libc, 'renamex_np'):
        raise RuntimeError(f'macOS libc "{libc_name}" is missing'
                            ' renamex_np() system call.')
    RENAME_EXCL = 0x4
    def renamex_np_noreplace_closure(src, dest):
        rc = libc.renamex_np(src.encode('UTF-8'),
                             dest.encode('UTF-8'),
                             RENAME_EXCL)
        if rc == 0:
            return
        if rc == -1:
            _errno = ctypes.get_errno()
            # Raising `OSError` seems to actually raise the appropriate
            #  exception type according to the passed-in `errno`.  Of concern
            #  to us is `EEXIST`, for which it will raise `FileExistsError`.
            raise OSError(_errno, os.strerror(_errno), src, None, dest)
        raise RuntimeError(f'Unexpected rcode {rc} from renamex_np() for'
                           f' renaming "{src}" to "{dest}".')
    logger.info('No-replace rename implemented as:'
                ' macOS syscall renamex_np(RENAME_EXCL)')
    return renamex_np_noreplace_closure

# Creating a hard link with the destination name, then unlinking the old name,
#  is mostly safe.  It won't clobber the destination; and it doesn't "rename"
#  by actually making a copy, so won't consume extra diskspace and the file
#  modtime will be preserved.  However, if unlinking the source file fails
#  after linking the destination file succeeded, then you end up with an
#  unwanted hard link, and attempting to clean up by unlinking the destination
#  file is just another step that could potentially fail.
def __get_rename_noreplace_fallback(exc=None):
    logger = logging.getLogger('__get_rename_noreplace_fallback')
    if exc:
        logger.warning('Initializing fallback no-replace rename because failed'
                      f' to initialize platform-specific one: {repr(exc)}')
    else:
        logger.warning('Initializing fallback no-replace rename because there'
                       ' is no platform-specific one for current platform.')
    def link_unlink(src, dest):
        os.link(src, dest)
        os.unlink(src)
    logger.info('No-replace rename implemented as: link+unlink')
    return link_unlink

try:
    rename_noreplace = {
        'Windows': __get_rename_noreplace_for_Windows,
        'Linux':   __get_rename_noreplace_for_Linux,
        'Darwin':  __get_rename_noreplace_for_macOS
    }.get(platform.system(), __get_rename_noreplace_fallback)()
except Exception as e:
    rename_noreplace = __get_rename_noreplace_fallback(e)

def try_rename_noreplace(src, dest):
    try:
        rename_noreplace(src, dest)
        return True
    except FileExistsError:
        return False

def rename_with_single_backup(src, dest):
    '''
    Rename dest to add ".bak" extension (clobbering any existing backup), then
    rename src to dest.
    '''
    os.replace(dest, f'{dest}.bak')
    rename_noreplace(src, dest)

def rename_with_backup(src, dest):
    '''Rename dest to add unique extension, then rename src to dest.'''
    def utc_timestamp_sfx():
        return datetime.datetime.now(datetime.timezone.utc).strftime('%Y%m%d%H%M%S')
    # NOTE: Using PID in backup file's name is for informational purposes only,
    #        to aid in debugging to figure out which current or past process
    #        created a particular backup file.
    pid = os.getpid()
    while not try_rename_noreplace(dest,
                                   f'{dest}.bak.{utc_timestamp_sfx()}.{pid}'):
        pass
    rename_noreplace(src, dest)

## CONVENTIONAL SAFE FILE-HANDLING UTILITY FUNCTIONS: #########################
## (only safe if all relevant parties follow same convention) #################

class ClaimedFile(LoggerMixin):

    def __init__(self, file_ctxmgr=None, *, lock_file_name=None):
        if callable(getattr(file_ctxmgr, 'filename', None)):
            self.__lock_file_name = f'{file_ctxmgr.filename()}.lck'
        else:
            if lock_file_name is None:
                raise RuntimeError('Cannot determine lock file name because'
                                   ' target file CM lacks a \'filename\''
                                   ' attribute and lock file name was not'
                                   ' specified.')
            self.__lock_file_name = lock_file_name
        LoggerMixin.__init__(self, f'"{self.__lock_file_name}"')
        self.__lock_file_ctxmgr = None
        self.__file_ctxmgr = file_ctxmgr
        self.__pid = os.getpid()

    def __enter__(self):
        self._logger().debug('Opening lock file for exclusive creation and'
                             ' writing.')
        self.__lock_file_ctxmgr = open(self.__lock_file_name,
                                       'x',
                                       encoding='UTF-8')
        self._logger().debug('Entering lock file CM.')
        lock_file = self.__lock_file_ctxmgr.__enter__()
        # NOTE: Writing PID is for informational purposes only.  It won't be
        #        programatically verified by anything afterwards, and if all
        #        parties that could potentially access a given file are using
        #        this exact mechanism, it shouldn't be necessary to verify.
        #        The PID is just there to aid in debugging to figure out which
        #        current or past process created a particular lock file, for
        #        instance if a program is long-running and still has a file
        #        locked, or if a program crashes before removing a lock file.
        print(f'Locked by PID {self.__pid}.', file=lock_file, flush=True)
        if self.__file_ctxmgr is None:
            return None
        self._logger().debug('Entering target file CM.')
        return self.__file_ctxmgr.__enter__()

    def __exit__(self, *exc_details):
        exception_was_raised = any(x is not None for x in exc_details)
        exc = exc_details[1]
        if exception_was_raised:
            self._logger().debug(f'Exiting with exception: {repr(exc)}')
        else:
            self._logger().debug('Exiting normally.')

        if self.__lock_file_ctxmgr:
            # If lock file's context manager (first bit of state initialized in
            #  __init__()) is set, that means we have some cleaning up to do.

            suppress_exception = False

            # First, clean up target file's context manager (reverse of
            #  initialization order) if there was one (caller may have used
            #  this class just to manage a lock file and not a target file).
            if self.__file_ctxmgr is not None:
                self._logger().debug('Exiting target file CM.')
                if self.__file_ctxmgr.__exit__(*exc_details):
                    self._logger().debug('Exiting target file CM suppressed'
                                         ' exception.')
                    suppress_exception = True
                    exc_details = (None, None, None)
                self.__file_ctxmgr = None

            # Second, clean up lock file's context manager (reverse of
            #  initialization order).
            self._logger().debug('Exiting lock file CM.')
            if self.__lock_file_ctxmgr.__exit__(*exc_details):
                self._logger().debug('Exiting lock file CM suppressed'
                                     ' exception.')
                suppress_exception = True
                exc_details = (None, None, None)
            self.__lock_file_ctxmgr = None

            # Finally, clean up lock file itself.
            self._logger().debug('Deleting lock file.')
            try:
                os.remove(self.__lock_file_name)
            except Exception as e:
                self._logger().error(f'Failed to delete lock file: {repr(e)}')

            self._logger().debug(('Done, will suppress exception.'
                                  if suppress_exception else
                                  'Done, will allow exception to propagate.')
                                 if exception_was_raised else
                                 'Done')
            return exception_was_raised and suppress_exception

        # If lock file's context manager (first bit of state initialized in
        #  __init__()) wasn't set, that means an exception was raised from
        #  __init__(), or from __enter__() while lock file was being opened.
        self._logger().debug('Done, will allow exception to propagate.')
        assert exception_was_raised
        return False

class BaseWriteFile(abc.ABC, LoggerMixin):

    @staticmethod
    def __valid_filename(filename):
        if len(filename) == 0:
            raise ValueError('filename cannot be blank')
        if os.path.basename(filename) == '':
            raise ValueError('filename cannot refer to a directory (end with'
                            f' {filename[-1]}): "{filename}"')
        return filename

    def __init__(self,
                 filename,
                 *,
                 binary=False,
             # params passed through to `open()` for target/proxy file(s):
                 buffering=-1,
                 encoding=None,
                 errors=None,
                 newline=None):
        LoggerMixin.__init__(self, f'"{filename}"')
        self._filename = self.__valid_filename(filename)
        self._binary_mode = 'b' if binary else ''
        self._open_kwargs = {'buffering': buffering,
                             'encoding':  encoding,
                             'errors':    errors,
                             'newline':   newline}

    def filename(self):
        return self._filename

    @abc.abstractmethod
    def __enter__(self):
        pass

    @abc.abstractmethod
    def __exit__(self, *exc_details):
        pass

class WriteInPlaceFile(BaseWriteFile):

    def __init__(self,
                 filename,
                 *,
                 must_already_exist=False,
                 overwrite=False,
                 **open_kwargs):
        super().__init__(filename, **open_kwargs)
        self.__must_already_exist = must_already_exist
        self.__overwrite = overwrite
        self.__file_ctxmgr = None

    def __enter__(self):
        try:
            # Attempt to open target file in read+write mode.
            # NOTE: You could use the lower level `os.open()` in order to make
            #        use of lower level file modes like `os.O_RDWR|os.O_TRUNC`,
            #        but then the returned object would be a file descriptor,
            #        not a context manager.  So instead you use `open()` below
            #        with 'r+' for read+write mode (since that's the only
            #        write-capable mode you can pass to `open()` that will
            #        require the file to exist) and manually handle the
            #        resetting of position and truncation yourself.
            self._logger().debug('Opening target file for reading and'
                                 ' writing.')
            self.__file_ctxmgr = open(self._filename,
                                      f'r+{self._binary_mode}',
                                      **self._open_kwargs)
            self._logger().debug('Entering target file CM.')
            file = self.__file_ctxmgr.__enter__()
            if self.__overwrite:
                self._logger().debug('Truncating existing target file to 0'
                                     ' bytes.')
                file.truncate()
            if self.__must_already_exist:
                # If caller required target file to already exist (and it did,
                #  b/c otherwise FileNotFoundError would've been raised), then
                #  there's no point in returning bool of whether target file
                #  already existed b/c it'll always be true, so return just the
                #  target file object.
                return file
            return file, True # True: target file already existed
        except FileNotFoundError:
            if self.__must_already_exist:
                # If caller required target file to already exist (and it
                #  didn't, b/c FileNotFoundError was raised), then re-raise
                #  exception.
                raise
            # Target file didn't already exist, so create it anew and open that
            #  instead.
            self._logger().debug('Target file didn\'t exist, so instead'
                                 ' opening it for exclusive creation and'
                                 ' writing.')
            self.__file_ctxmgr = open(self._filename,
                                      f'x{self._binary_mode}',
                                      **self._open_kwargs)
            self._logger().debug('Entering target file CM.')
            return (self.__file_ctxmgr.__enter__(),
                    False) # False: target file didn't already exist

    def __exit__(self, *exc_details):
        exception_was_raised = any(x is not None for x in exc_details)
        exc = exc_details[1]
        if exception_was_raised:
            self._logger().debug(f'Exiting with exception: {repr(exc)}')
        else:
            self._logger().debug('Exiting normally.')

        if self.__file_ctxmgr:
            # If target file's context manager (first bit of state initialized
            #  in __init__()) is set, that means we have some cleaning up to
            #  do.

            suppress_exception = False

            # Clean up target file's context manager.
            self._logger().debug('Exiting target file CM.')
            if self.__file_ctxmgr.__exit__(*exc_details):
                self._logger().debug('Exiting target file CM suppressed'
                                     ' exception.')
                suppress_exception = True
                exc_details = (None, None, None)
            self.__file_ctxmgr = None

            self._logger().debug(('Done, will suppress exception.'
                                  if suppress_exception else
                                  'Done, will allow exception to propagate.')
                                 if exception_was_raised else
                                 'Done')
            return exception_was_raised and suppress_exception

        # If target file's context manager (first bit of state initialized in
        #  __init__()) wasn't set, that means an exception was raised from
        #  __init__(), or from __enter__() while target file was being opened.
        self._logger().debug('Done, will allow exception to propagate.')
        assert exception_was_raised
        return False

class AbortProxyWrite(Exception):
    def __init__(self, delete_write_proxy_file=True):
        self.__delete_write_proxy_file = delete_write_proxy_file
    def should_delete_write_proxy_file(self):
        return self.__delete_write_proxy_file

class WriteProxiedFile(BaseWriteFile):

    @staticmethod
    def __valid_num_backups(num_backups):
        VALID_NUM_BACKUPS_VALUES = (0, 1, float('inf'))
        if num_backups not in VALID_NUM_BACKUPS_VALUES:
            raise ValueError('num_backups must be one of:'
                            f' {", ".join(VALID_NUM_BACKUPS_VALUES)}')
        return num_backups

    def __init__(self,
                 filename,
                 *,
                 must_already_exist=False,
                 num_backups=0, # supports 0, 1, and inf (i.e. `float('inf')` or `math.inf`)
                 delete_write_proxy_file_on_failure=True,
                 **open_kwargs):
        super().__init__(filename, **open_kwargs)
        self.__must_already_exist = must_already_exist
        self.__num_backups = self.__valid_num_backups(num_backups)
        self.__delete_write_proxy_file_on_failure = delete_write_proxy_file_on_failure
        self.__write_proxy_file_ctxmgr = None
        self.__file_ctxmgr = None

    def __enter__(self):
        # Open write-proxy file in exclusive-create mode.
        self._logger().debug('Opening write-proxy file for exclusive creation'
                             ' and writing.')
        self.__write_proxy_file_ctxmgr = open(f'{self._filename}.tmp',
                                              f'x{self._binary_mode}',
                                              **self._open_kwargs)
        try:
            # Attempt to open target file in read-only mode.
            self._logger().debug('Opening target file for reading.')
            self.__file_ctxmgr = open(f'{self._filename}',
                                      f'r{self._binary_mode}',
                                      **self._open_kwargs)
            self._logger().debug('Entering target file CM and write-proxy file'
                                 ' CM.')
            return (self.__file_ctxmgr.__enter__(),
                    self.__write_proxy_file_ctxmgr.__enter__())
        except FileNotFoundError:
            if self.__must_already_exist:
                # Caller required target file to already exist, so re-raise the
                #  FileNotFoundError.
                raise
            # Caller didn't require target file to already exist, so just leave
            #  its context manager as None, and return None instead of the
            #  opened target file object.
            self._logger().debug('Target file didn\'t exist.')
            self.__file_ctxmgr = None
            self._logger().debug('Entering write-proxy file CM.')
            return None, self.__write_proxy_file_ctxmgr.__enter__()

    def __exit__(self, *exc_details):
        exception_was_raised = any(x is not None for x in exc_details)
        exc = exc_details[1]
        if exception_was_raised:
            self._logger().debug(f'Exiting with exception: {repr(exc)}')
        else:
            self._logger().debug('Exiting normally.')

        if self.__write_proxy_file_ctxmgr:
            # If write-proxy file's context manager (first bit of state
            #  initialized in __init__()) is set, that means we have some
            #  cleaning up to do.

            suppress_exception = False

            # First, clean up target file's context manager (reverse of
            #  initialization order).
            if self.__file_ctxmgr:
                # If target file's context manager is set, clean it up.
                file_already_existed = True
                self._logger().debug('Exiting target file CM.')
                if self.__file_ctxmgr.__exit__(*exc_details):
                    self._logger().debug('Exiting target file CM suppressed'
                                         ' exception.')
                    suppress_exception = True
                    exc_details = (None, None, None)
                self.__file_ctxmgr = None
            else:
                # If target file's context manager wasn't set, and given the
                #  other conditions above, that means either an exception was
                #  raised from __enter__() while target file was being opened,
                #  or target file didn't already exist.
                if not exception_was_raised:
                    # If no exception was raised, and given the other
                    #  conditions above, then target file must not have already
                    #  existed, and therefore also caller must not have
                    #  required it to already exist.
                    file_already_existed = False
                    assert not self.__must_already_exist
                # NOTE: If exception_was_raised, we deliberately don't declare
                #        file_already_existed because we don't know whether the
                #        target file already existed, but that's fine because
                #        we don't check file_already_existed below if
                #        exception_was_raised.

            # Second, clean up write-proxy file's context manager (reverse of
            #  initialization order).
            self._logger().debug('Exiting write-proxy file CM.')
            if self.__write_proxy_file_ctxmgr.__exit__(*exc_details):
                self._logger().debug('Exiting write-proxy file CM suppressed'
                                     ' exception.')
                suppress_exception = True
                exc_details = (None, None, None)
            self.__write_proxy_file_ctxmgr = None

            # Finally, reconcile write-proxy file with target file.
            if exception_was_raised:
                # Something went wrong (exception was raised), or caller just
                #  decided to abort creating/replacing target file
                #  (AbortProxyWrite was raised).  Discard write-proxy file
                #  unless instructed otherwise.
                if isinstance(exc, AbortProxyWrite):
                    suppress_exception = True
                    delete_write_proxy_file = exc.should_delete_write_proxy_file()
                else:
                    delete_write_proxy_file = self.__delete_write_proxy_file_on_failure
                if delete_write_proxy_file:
                    self._logger().debug('Deleting write-proxy file.')
                    try:
                        os.remove(f'{self._filename}.tmp')
                    except Exception as e:
                        self._logger().error('Failed to delete write-proxy'
                                            f' file: {repr(e)}')
            else:
                # Writes completed successfully, so follow through on whatever
                #  strategy was requested.
                if file_already_existed:
                    # If target file already existed, then we should either:
                    #  * rename target file to a backup name, then rename
                    #     write-proxy file to target file (and expect to not be
                    #     replacing an existing target file)
                    #  * replace target file with write-proxy file
                    self._logger().debug('Replacing target file with'
                                         ' write-proxy file, keeping'
                                        f' {self.__num_backups} backup(s).')
                    if self.__num_backups == float('inf'):
                        rename_with_backup(f'{self._filename}.tmp',
                                           self._filename)
                    elif self.__num_backups == 1:
                        rename_with_single_backup(f'{self._filename}.tmp',
                                                  self._filename)
                    else:
                        assert self.__num_backups == 0
                        os.replace(f'{self._filename}.tmp', self._filename)
                        # NOTE: `os.replace()` will clobber dest (if necessary)
                        #        on all platforms, whereas `os.rename()` only
                        #        does so on POSIX platforms.
                else:
                    # If target file didn't already exist, then we should
                    #  rename write-proxy file to target file (and expect to
                    #  not be replacing an existing target file).
                    self._logger().debug('Renaming write-proxy file to target'
                                         ' file.')
                    rename_noreplace(f'{self._filename}.tmp', self._filename)

            self._logger().debug(('Done, will suppress exception.'
                                  if suppress_exception else
                                  'Done, will allow exception to propagate.')
                                 if exception_was_raised else
                                 'Done')
            return exception_was_raised and suppress_exception

        # If write-proxy file's context manager (first bit of state initialized
        #  in __init__()) wasn't set, that means an exception was raised from
        #  __init__(), or from __enter__() while write-proxy file was being
        #  opened.
        self._logger().debug('Done, will allow exception to propagate.')
        assert exception_was_raised
        return False

class RedirectStdIO:

    def __init__(self, std_fd, target_file_path, flags=None):

        # validate std_fd, then do some init
        if callable(getattr(std_fd, 'fileno', None)):
            std_fd = std_fd.fileno()
        if std_fd not in (0, 1, 2):
            raise ValueError('std_fd must be stdin, stdout, or stderr')
        self.__std_fd = std_fd

        # do some init
        self.__target_file_path = target_file_path

        # validate flags, then do some init
        if flags is None:
            if self.__std_fd == 0:
                self.__flags = os.O_RDONLY
            else:
                self.__flags = os.O_WRONLY
                if self.__target_file_path != os.devnull:
                    self.__flags |= os.O_APPEND | os.O_CREAT
        else:
            # NOTE: `man open` explains that unlike the other flags, O_RDONLY,
            #        O_WRONLY, and O_RDWR are not actually flags, but in fact
            #        each one is a value for the entire low order 2 bits. So,
            #        they cannot be checked for by simply masking `flags` like
            #        `if flags & os.O_RDONLY`, those low order bits must be
            #        extracted and compared directly to `O_RDONLY`, etc. for
            #        equality.
            flags_loworder_bits = flags & 0b11
            if self.__std_fd == 0:
                if flags_loworder_bits not in (os.O_RDONLY, os.O_RDWR):
                    raise ValueError('flags must be O_RDONLY or O_RDWR when'
                                     ' std_fd is stdin')
            else:
                if flags_loworder_bits not in (os.O_WRONLY, os.O_RDWR):
                    raise ValueError('flags must be O_WRONLY or O_RDWR when'
                                     ' std_fd is stdout or stderr')
            self.__flags = flags

        # do some init
        self.__target_file_fd = None
        self.__std_backup_fd = None

    def __enter__(self):

        # open target file which we'll be redirecting std fd to/from, i.e. get
        #  fd pointing to file description of opened target file
        self.__target_file_fd = os.open(self.__target_file_path, self.__flags)

        # create new "backup" fd pointing to same file description that std fd
        #  is pointing to
        self.__std_backup_fd = os.dup(self.__std_fd)

        # redirect std fd to point to file description of opened target file
        os.dup2(self.__target_file_fd, self.__std_fd)

        return None

    def __exit__(self, *exc_details):
        if self.__std_backup_fd is not None:

            # redirect std fd to point back to file description it had pointed
            #  to originally
            os.dup2(self.__std_backup_fd, self.__std_fd)

            # close backup fd, but since std fd now points to same file
            #  description, it won't actually close file, just removes backup
            #  fd from fd table
            os.close(self.__std_backup_fd)

        if self.__target_file_fd is not None:

            # close target file fd, and since now no other fd points to target
            #  file description, it will actually close target file in addition
            #  to removing target file fd from fd table
            os.close(self.__target_file_fd)

        return False

###############################################################################

def read_config(config_file):
    config = json.load(config_file)
    duplicate_urls = {playlist_url
                      for playlist_url, count
                       in collections.Counter(
                                  playlist_info['url']
                                  for playlist_info
                                   in config['playlist_infos']).items()
                       if count > 1}
    if len(duplicate_urls) > 0:
        raise RuntimeError('Duplicate `url` values found in config file'
                          f' "{os.path.basename(config_file.name)}":'
                          f' {", ".join(duplicate_urls)}')
    return config

def read_state(state_file):
    state = {} if state_file is None else json.load(state_file)

    # read video IDs acknowledged by user before last report
    video_ids_acked_before_last_report = None
    if 'pre-report' in state:
        video_ids_acked_before_last_report = {}
        duplicate_urls = set()
        for playlist_pre_report_state in state['pre-report']:
            playlist_url = playlist_pre_report_state['playlist_url']
            if playlist_url in video_ids_acked_before_last_report:
                duplicate_urls.add(playlist_url)
                continue
            video_ids_acked_before_last_report[playlist_url] \
                    = set(playlist_pre_report_state['acked_video_ids']
                                  .split(','))
        if len(duplicate_urls) > 0:
            raise RuntimeError('Duplicate `playlist_url` values found under'
                               ' "pre-report" in state file'
                              f' "{os.path.basename(state_file.name)}":'
                              f' {", ".join(duplicate_urls)}')

    # read data that went into last report
    last_report_data = None
    if 'report' in state:
        last_report_data = {}
        duplicate_urls = set()
        for playlist_report_state in state['report']:
            playlist_url = playlist_report_state['playlist_url']
            if playlist_url in last_report_data:
                duplicate_urls.add(playlist_url)
                continue
            playlist_report_data = last_report_data.setdefault(playlist_url,
                                                               {})
            playlist_report_data['playlist_name'] \
                    = playlist_report_state['playlist_name']
            if 'notices' in playlist_report_state:
                playlist_report_data['notices'] \
                        = playlist_report_state['notices']
            if 'videos_older_to_newer' in playlist_report_state:
                playlist_report_data['videos_older_to_newer'] \
                        = playlist_report_state['videos_older_to_newer']
        if len(duplicate_urls) > 0:
            raise RuntimeError('Duplicate `playlist_url` values found under'
                               ' "report" in state file'
                              f' "{os.path.basename(state_file.name)}":'
                              f' {", ".join(duplicate_urls)}')

    return video_ids_acked_before_last_report, last_report_data

def write_config(config_file, config):
    json.dump(config,
              config_file,
              ensure_ascii=False, # don't escape Unicode chars, leave them as-is
              indent=4)
    print(file=config_file) # json.dump doesn't print final line terminator

def write_state(state_file, acked_video_ids, report_data):
    def gen_playlist_report_state(playlist_url, playlist_report_data):
        playlist_report_state \
                = {'playlist_url':  playlist_url,
                   'playlist_name': playlist_report_data['playlist_name']}
        for field in ('notices', 'videos_older_to_newer'):
            if field in playlist_report_data:
                playlist_report_state[field] = playlist_report_data[field]
        return playlist_report_state
    pre_report_state = [{'playlist_url':    playlist_url,
                         'acked_video_ids': ','.join(sorted(playlist_acked_video_ids))}
                        for playlist_url, playlist_acked_video_ids
                         in acked_video_ids.items()]
    report_state = [gen_playlist_report_state(playlist_url,
                                              playlist_report_data)
                    for playlist_url, playlist_report_data
                     in report_data.items()]
    state = {}
    if pre_report_state:
        state['pre-report'] = pre_report_state
    if report_state:
        state['report'] = report_state
    json.dump(state,
              state_file,
              ensure_ascii=False, # don't escape Unicode chars, leave them as-is
              indent=4)
    print(file=state_file) # json.dump doesn't print final line terminator

def get_playlist_report_data(playlist_report_data, # input/output param
                             playlist_acked_video_ids,
                             playlist_url,
                             playlist_name=None):
    logger = logging.getLogger('get_playlist_report_data')
    if playlist_name is None:
        playlist_name_text = ''
    else:
        playlist_name_text = f' ("{playlist_name}")'
    already_known_video_ids = copy.deepcopy(playlist_acked_video_ids)
    if 'videos_older_to_newer' in playlist_report_data:
        already_known_video_ids \
                |= {v['id']
                    for v in playlist_report_data['videos_older_to_newer']}
    logger.info(f'Downloading playlist at {playlist_url}{playlist_name_text}'
                 ' (may take several minutes if many videos):')
    with PausedLogFile(logger) as lfn:
        with open(lfn, 'r+', encoding='UTF-8') as lf:
            lf.seek(0, io.SEEK_END)
            playlist = json.loads(subprocess.run(['yt-dlp', '--ignore-config',
                                                            '--no-cache-dir',
                                                            '--verbose',
                                                            '--flat-playlist',
                                                                # not documented very well, but it seems this will internally
                                                                #  download only playlist pages, not each video's page, which
                                                                #  should be faster but will also limit what metadata is
                                                                #  available per video
                                                            '--dump-single-json',
                                                                # dump video data to stdout in JSON, but unlike --dump-json,
                                                                #  this will output single JSON structure for entire retrieved
                                                                #  subset of playlist, including metadata about playlist itself
                                                            '--',
                                                            playlist_url],
                                                            # yt-dlp supports querying for only videos uploaded after a
                                                            #  specified date, but this feature has a few critical flaws
                                                            #  preventing it from being used here.  For channels' or users'
                                                            #  "/videos", "/streams", or "/shorts" pages, it cannot get exact
                                                            #  upload dates for videos when using --flat-playlist, it can only
                                                            #  give approximate upload dates, and only if you also pass:
                                                            #       --extractor-args youtubetab:approximate_date
                                                            #  The approximations are calculated from text like "1 day ago",
                                                            #  "3 months ago", "2 years ago", etc.  As you can see the
                                                            #  precision drops dramatically as the video gets older.  For
                                                            #  playlist ID URLs (i.e. https://.../playlist?list=...) it can't
                                                            #  get upload dates at all, not even approximate ones.  Even when
                                                            #  dealing with channels/users that update frequently and have
                                                            #  done so recently, there is some instability in the upload date
                                                            #  reported by yt-dlp, leading to unreliable reporting.
                                                            # Playlist maintainers (and YouTube itself) can also remove or
                                                            #  hide videos, and later restore them, and this can also subvert
                                                            #  the expectations of this script.  yt-dlp may also unexpectedly
                                                            #  be missing certain videos in its output, listing them sometimes
                                                            #  but inexplicably omitting them other times -- for example this
                                                            #  happened when YouTube was transitioning from having all content
                                                            #  (videos, live streams, and shorts) listed together on the
                                                            #  "/videos" page to having them on separate pages.
                                                            # So the most reliable technique is in fact to not filter with
                                                            #  yt-dlp but instead record the full list of IDs it returns, and
                                                            #  on every subsequent run merge any new IDs into that list but
                                                            #  never remove any, which is what we do below.
                                                 stdout=subprocess.PIPE, # stdout is captured
                                                 stderr=lf, # stderr goes to log file
                                                 check=True,
                                                 encoding='UTF-8').stdout)
    logger.info('Playlist download complete, constructing report data...')
    if playlist_name is None:
        playlist_name = playlist['title']
    playlist_report_data['playlist_name'] = playlist_name
    if len(playlist['entries']) > 0:
        encountered_already_known_id = False
        videos_older_to_newer = collections.deque()
        for video in playlist['entries']:
            if video['id'] in already_known_video_ids:
                encountered_already_known_id = True
                break
            videos_older_to_newer.appendleft(
                    {'id':          video['id'],
                     'title':       video['title'],
                     'description': video['description']})
        if (    len(already_known_video_ids) > 0
            and not encountered_already_known_id):
            playlist_report_data.setdefault('notices', set()).add(
                    'No previously known video IDs were encountered during'
                    ' this run, so one or more videos may be missing in list'
                    ' below!')
        if len(videos_older_to_newer) > 0:
            playlist_report_data.setdefault('videos_older_to_newer',
                                            []).extend(videos_older_to_newer)
    logger.info('Playlist report data constructed.')
    return playlist_name

def write_report(report_file, report_data, report_start_timestamp):
    logger = logging.getLogger('write_report')
    logger.info('Writing report...')
    print('<html>\n'
         f'<head><title>YouTube Follow Report @ {report_start_timestamp.strftime("%c (%Z)")}</title></head>\n'
          '<body>',
          file=report_file)
    for playlist_url, playlist_report_data in report_data.items():
        # NOTE: We use CSS `padding-left` below instead of `text-indent`
        #        because the latter only indents the first contained line and
        #        we want to indent all the lines together.
        print(f'<h2><a href="{playlist_url}">{html.escape(playlist_report_data["playlist_name"])}</a></h2>\n'
               '    <div style="padding-left: 50px;">\n'
               '    New videos, older to newer:',
              file=report_file)
        for notice in playlist_report_data.get('notices', []):
            print(f'    <p><b>WARNING:</b> {html.escape(notice)}</p>',
                  file=report_file)
        for video in playlist_report_data.get('videos_older_to_newer', []):
            # NOTE: The following HTML code for embedding a YouTube video was
            #        taken from right-clicking a video on YouTube, selecting
            #        "Copy embed code", tweaking the size, and replacing static
            #        data with variables:
            html_title       = html.escape(video['title'])
            html_description = html.escape(video['description']).replace('\n', '&#10;')
            print(f'    <p><a href="https://www.youtube.com/watch?v={video["id"]}"\n'
                  f'          title="{html_description}">{html_title}</a><br/>\n'
                   '       <iframe width="480"\n'
                   '               height="360"\n'
                  f'               src="https://www.youtube.com/embed/{video["id"]}"\n'
                  f'               title="{video["title"]}"\n'
                   '               frameborder="0"\n'
                   '               allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"\n'
                   '               allowfullscreen></iframe></p>',
                  file=report_file)
        print('    </div>', file=report_file)
    if len(report_data) == 0:
        print('No new videos.', file=report_file)
    print('</body>\n'
          '</html>',
          file=report_file)
    logger.info('Report written.')

def use_report(report_file_name,
               *,
               launch=False,
               prompt_to_delete=False,
               delete=None):
    logger = logging.getLogger('use_report')
    if delete is None:
        delete = prompt_to_delete
    elif prompt_to_delete and not delete:
        raise ValueError('cannot have prompt_to_delete be True while delete is'
                         ' False')
    if not os.path.exists(report_file_name):
        logger.info(f'Report file "{report_file_name}" not found, nothing to'
                     ' do.')
        return
    with ClaimedFile(lock_file_name=f'{report_file_name}.lck'):
        if launch:
            logger.info(f'Launching report file "{report_file_name}" in web'
                         ' browser.')
            file_url = pathlib.Path(report_file_name).absolute().as_uri()

            # webbrowser module may produce output, and provides no direct
            #  means of suppressing it, so we just redirect stdout+stderr to
            #  /dev/null (or Windows' equivalent) for duration of open_new_tab
            with RedirectStdIO(sys.stdout, os.devnull), \
                 RedirectStdIO(sys.stderr, os.devnull):
                webbrowser.open_new_tab(file_url)

        if prompt_to_delete:
            recognized_responses = ('yes', 'no', 'y', 'n')
            while (response := input('Delete report file'
                                    f' "{report_file_name}"? ').strip().lower()) \
                  not in recognized_responses:
                print('Response not recognized.  It must be one of:'
                     f' {", ".join(recognized_responses)}')
            if response.startswith('n'):
                return
        if delete:
            logger.info(f'Deleting report file "{report_file_name}".')
            os.remove(report_file_name)

def main__usage():
    print(USAGE.format(program=os.path.basename(sys.argv[0])))
    return 1

def main__generate_report():
    # init
    program_start_timestamp_utc = datetime.datetime.now(datetime.timezone.utc)

    # setup logging
    try:
        os.remove(LOG_FILE_NAME)
    except FileNotFoundError:
        pass
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    fh = logging.FileHandler(LOG_FILE_NAME, mode='x', encoding='UTF-8')
    fh.setLevel(logging.DEBUG)
    class UTCFormatter(logging.Formatter):
        converter = time.gmtime
    fmtr = UTCFormatter('[%(asctime)s.%(msecs)03dZ]'
                        ' %(levelname)s: %(name)s: %(message)s',
                        datefmt='%Y-%m-%dT%H:%M:%S')
    fh.setFormatter(fmtr)
    root_logger.addHandler(fh)
    print(f'See "{LOG_FILE_NAME}" to monitor progress.')
    logger = logging.getLogger('main')
    logger.info(f'Program started as PID {os.getpid()} at'
                f' {program_start_timestamp_utc}, using config file'
                f' "{CONFIG_FILE_NAME}", state file "{STATE_FILE_NAME}", and'
                f' report file "{REPORT_FILE_NAME}".')

    # init/read files
    with ClaimedFile(WriteProxiedFile(STATE_FILE_NAME, encoding='UTF-8')) \
            as (state_file, state_write_proxy_file):
        with ClaimedFile(WriteProxiedFile(CONFIG_FILE_NAME,
                                          must_already_exist=True,
                                          num_backups=float('inf'),
                                          encoding='UTF-8')) \
                as (config_file, config_write_proxy_file):
            config_modified = False
            config = read_config(config_file)
            with ClaimedFile(WriteProxiedFile(REPORT_FILE_NAME,
                                              encoding='UTF-8')) \
                    as (report_file, report_write_proxy_file):
                (video_ids_acked_before_last_report,
                 last_report_data) = read_state(state_file)
                # NOTE: Later, when writing new state, we'll compare it to the
                #        state we just read, so in both cases below we must
                #        deep copy anything which will/may get modified.
                if report_file is None:
                    # No report file exists, which should mean either:
                    #   * This program is running for the first time (in the
                    #      current directory), which means there's also no
                    #      state file yet, so the state object we have now is
                    #      empty.
                    #   * The user already viewed and deleted it, so we need to
                    #      create a new one using the state as it was after
                    #      that deleted report was generated (which we'll
                    #      construct by adding the IDs in last report's data to
                    #      the IDs that the user had acknowledged before the
                    #      last report was generated).
                    logger.debug('Report file not found, so set of user-acked'
                                 ' video IDs = set of video IDs acked before'
                                 ' last report (if any) + set of video IDs'
                                 ' used in last report (if any), and new'
                                 ' report\'s data starts out empty.')
                    acked_video_ids = (
                            {} if video_ids_acked_before_last_report is None
                            else copy.deepcopy(
                                         video_ids_acked_before_last_report))
                    if last_report_data is not None:
                        for (playlist_url,
                             playlist_report_data) in last_report_data.items():
                            acked_video_ids.setdefault(playlist_url,
                                                       set()).update(
                                    {v['id']
                                     for v
                                      in playlist_report_data[
                                                 'videos_older_to_newer']})
                    new_report_data = {}
                else:
                    # A report file already exists, so we need to replace it
                    #  with a fresh one that includes everything currently in
                    #  it, plus anything new since.
                    logger.debug('Report file found, so set of user-acked'
                                 ' video IDs = set of video IDs acked before'
                                 ' last report (if any), and new report\'s'
                                 ' data starts out with last report\'s data.')
                    acked_video_ids = (
                            {} if video_ids_acked_before_last_report is None
                            else video_ids_acked_before_last_report)
                    new_report_data = (
                            {} if last_report_data is None
                            else copy.deepcopy(last_report_data))

                # gather info
                for playlist_info in config['playlist_infos']:
                    playlist_url = playlist_info['url']
                    playlist_acked_video_ids \
                            = acked_video_ids.get(playlist_url, set())
                    playlist_new_report_data \
                            = new_report_data.setdefault(playlist_url, {})
                    playlist_name_in_config = playlist_info.get('name', None)
                    get_playlist_report_data(playlist_new_report_data,
                                             playlist_acked_video_ids,
                                             playlist_url,
                                             playlist_name_in_config)
                    if playlist_name_in_config is None:
                        playlist_name_in_config \
                                = playlist_info['name'] \
                                = playlist_new_report_data['playlist_name']
                        config_modified = True
                    if not any(field in playlist_new_report_data
                               for field in ('notices',
                                             'videos_older_to_newer')):
                        # NOTE: There are no notices or videos in this
                        #        playlist's report data, so we must delete its
                        #        entry (which we must've just recently created
                        #        when we called `.setdefault(...)` on
                        #        `new_report_data` above) because the rest of
                        #        the code assumes that we only have entries for
                        #        playlists that actually have data to go into
                        #        the report (i.e. either notices or videos).
                        del new_report_data[playlist_url]

                # write report
                write_report(report_write_proxy_file,
                             new_report_data,
                             program_start_timestamp_utc)
                # If write_report or closing of report file's `with` block
                #  fails for any reason, exception is raised before we write
                #  actual report file, so we'd also skip logic below for
                #  updating playlist names in config file (no big deal) and
                #  writing updated state file, thus in the end none of the
                #  files get written and it's as if program just never ran,
                #  which is exactly the transactional behavior we want.
                # If report existed at start, but was removed by time of
                #  writing (for instance because user had opened report without
                #  using this program's "view" mode, watched videos, then
                #  deleted report, all while this program was running) then
                #  newly written report will contain videos user already saw
                #  (because report had existed at the time this program
                #  started).  While this may be annoying, it is at least better
                #  than alternative of erring in favor of skipping videos user
                #  may not have seen.  To prevent this from happening, user
                #  should only view/delete report by using this program's
                #  "view" mode.

            # write config, if it's been modified
            if not config_modified:
                raise AbortProxyWrite
            write_config(config_write_proxy_file, config)
            # If write_config or closing of config file's `with` block fails
            #  for any reason, it's not a big deal because new config (which
            #  failed to write) just contained some names of playlists that had
            #  names unset in original config (not a big deal).

        # write state, if it's been modified
        if (    (acked_video_ids if acked_video_ids else None) == video_ids_acked_before_last_report
            and (new_report_data if new_report_data else None) == last_report_data):
            raise AbortProxyWrite
        write_state(state_write_proxy_file, acked_video_ids, new_report_data)
        # If write_state or closing of state file's `with` block fails for any
        #  reason, it's not a big deal because previous state file would still
        #  exist, and worst case user will be re-notified of some already-seen
        #  videos on next run of program.  If we had written state *before*
        #  writing report, a failed report write (after successful state write)
        #  would mean we might actually skip reporting some videos, which we
        #  definitely don't want to happen.

def main__view_report():
    # setup logging
    logging.basicConfig(stream=sys.stdout,
                        format='%(levelname)s: %(name)s: %(message)s',
                        level=logging.INFO)
    logger = logging.getLogger('main')
    logger.info(f'Program started as PID {os.getpid()},'
                f' using report file "{REPORT_FILE_NAME}".')

    # open report
    use_report(REPORT_FILE_NAME, launch=True, prompt_to_delete=True)

def main():
    args = sys.argv[1:]
    if len(args) != 1:
        return main__usage()
    return {
        'generate': main__generate_report,
        'view':     main__view_report
    }.get(args[0], main__usage)()

if __name__ == '__main__':
    sys.exit(main())
