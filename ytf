#!/usr/bin/env python3

# Copyright (c) 2022, cemysce
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

# YouTube Follow

import abc
import asyncio
import codecs
import collections
import copy
import datetime
import html
import inspect
import io
import json
import logging
import os
import pathlib
import platform
import signal
import string
import subprocess
import sys
import time
import webbrowser

assert sys.version_info >= (3, 8)

USAGE = '''ytf - YouTube Follow

Usage: {program} [generate|view]

  generate  Generates/regenerates report HTML file of new videos on followed
             playlists (logs to log file).

  view      Opens report HTML file in default web browser (logs to console).

Both modes lock report file to prevent concurrent use by other {program} instances.
'''

LOG_FILE_NAME    = 'ytf.log'
CONFIG_FILE_NAME = 'ytf-config.json'
STATE_FILE_NAME  = 'ytf-state.json'
REPORT_FILE_NAME = 'new-videos.html'

class LoggerMixin:
    def __init__(self, instance_identifying_text=None):
        self.__instance_identifying_text = instance_identifying_text
    def _logger(self):
        current_subclass_name = self.__class__.__name__
        current_frame = inspect.currentframe()
        calling_frame = None
        prefix_str = None
        try:
            calling_frame = current_frame.f_back
            calling_class_name = calling_frame.f_locals['self'].__class__.__name__
            assert current_subclass_name == calling_class_name
            calling_function_name = calling_frame.f_code.co_name
            instance_prefix = (f'{{{self.__instance_identifying_text}}}'
                               if self.__instance_identifying_text else
                               '')
            return logging.getLogger(f'{calling_class_name}{instance_prefix}'
                                     f'.{calling_function_name}')
        except:
            pass
        finally:
            # prevent keeping references to frame objects, since that can cause
            #  program to create reference cycles (see `inspect` module's
            #  documentation)
            del calling_frame
            del current_frame

###############################################################################
## GENERIC SAFE FILE-HANDLING UTILITY FUNCTIONS: ##############################

# Python Standard Library's `os.rename` won't clobber on Windows.
def __get_rename_noreplace_for_Windows():
    logger = logging.getLogger('__get_rename_noreplace_for_Windows')
    logger.info('Initializing no-replace rename for Windows.')
    # nothing to initialize...
    logger.info('No-replace rename implemented as: os.rename')
    return os.rename

# Linux's system call `renameat2(..., RENAME_NOREPLACE)` won't clobber and is
#  atomic.
def __get_rename_noreplace_for_Linux():
    logger = logging.getLogger('__get_rename_noreplace_for_Linux')
    logger.info('Initializing no-replace rename for Linux.')
    import ctypes
    import ctypes.util
    import errno
    libc_name = ctypes.util.find_library('c')
    if libc_name is None:
        raise RuntimeError('Linux libc not found.')
    libc = ctypes.CDLL(libc_name, use_errno=True)
    if not hasattr(libc, 'renameat2'):
        raise RuntimeError(f'Linux libc "{libc_name}" is missing'
                            ' renameat2() system call.')
    AT_FDCWD = -100
    RENAME_NOREPLACE = 1
    def renameat2_noreplace__closure(src, dest):
        rc = libc.renameat2(AT_FDCWD, src.encode('UTF-8'),
                            AT_FDCWD, dest.encode('UTF-8'),
                            RENAME_NOREPLACE)
        if rc == 0:
            return
        if rc == -1:
            _errno = ctypes.get_errno()
            # Raising `OSError` seems to actually raise the appropriate
            #  exception type according to the passed-in `errno`.  Of concern
            #  to us is `EEXIST`, for which it will raise `FileExistsError`.
            raise OSError(_errno, os.strerror(_errno), src, None, dest)
        raise RuntimeError(f'Unexpected rcode {rc} from renameat2() for'
                           f' renaming "{src}" to "{dest}".')
    logger.info('No-replace rename implemented as:'
                ' Linux syscall renameat2(RENAME_NOREPLACE)')
    return renameat2_noreplace__closure

# macOS's system call `renamex_np(..., RENAME_EXCL)` won't clobber and is
#  atomic.
def __get_rename_noreplace_for_macOS():
    logger = logging.getLogger('__get_rename_noreplace_for_macOS')
    logger.info('Initializing no-replace rename for macOS.')
    import ctypes
    import ctypes.util
    import errno
    libc_name = ctypes.util.find_library('c')
    if libc_name is None:
        raise RuntimeError('macOS libc not found.')
    libc = ctypes.CDLL(libc_name, use_errno=True)
    if not hasattr(libc, 'renamex_np'):
        raise RuntimeError(f'macOS libc "{libc_name}" is missing'
                            ' renamex_np() system call.')
    RENAME_EXCL = 0x4
    def renamex_np_noreplace__closure(src, dest):
        rc = libc.renamex_np(src.encode('UTF-8'),
                             dest.encode('UTF-8'),
                             RENAME_EXCL)
        if rc == 0:
            return
        if rc == -1:
            _errno = ctypes.get_errno()
            # Raising `OSError` seems to actually raise the appropriate
            #  exception type according to the passed-in `errno`.  Of concern
            #  to us is `EEXIST`, for which it will raise `FileExistsError`.
            raise OSError(_errno, os.strerror(_errno), src, None, dest)
        raise RuntimeError(f'Unexpected rcode {rc} from renamex_np() for'
                           f' renaming "{src}" to "{dest}".')
    logger.info('No-replace rename implemented as:'
                ' macOS syscall renamex_np(RENAME_EXCL)')
    return renamex_np_noreplace__closure

# Creating a hard link with the destination name, then unlinking the old name,
#  is mostly safe.  It won't clobber the destination; and it doesn't "rename"
#  by actually making a copy, so won't consume extra diskspace and the file
#  modtime will be preserved.  However, if unlinking the source file fails
#  after linking the destination file succeeded, then you end up with an
#  unwanted hard link, and attempting to clean up by unlinking the destination
#  file is just another step that could potentially fail.
def __get_rename_noreplace_fallback(exc=None):
    logger = logging.getLogger('__get_rename_noreplace_fallback')
    if exc:
        logger.warning('Initializing fallback no-replace rename because failed'
                      f' to initialize platform-specific one: {repr(exc)}')
    else:
        logger.warning('Initializing fallback no-replace rename because there'
                       ' is no platform-specific one for current platform.')
    def link_unlink(src, dest):
        os.link(src, dest)
        os.unlink(src)
    logger.info('No-replace rename implemented as: link+unlink')
    return link_unlink

try:
    rename_noreplace = {
        'Windows': __get_rename_noreplace_for_Windows,
        'Linux':   __get_rename_noreplace_for_Linux,
        'Darwin':  __get_rename_noreplace_for_macOS
    }.get(platform.system(), __get_rename_noreplace_fallback)()
except Exception as e:
    rename_noreplace = __get_rename_noreplace_fallback(e)

def try_rename_noreplace(src, dest):
    try:
        rename_noreplace(src, dest)
        return True
    except FileExistsError:
        return False

def rename_with_single_backup(src, dest):
    '''
    Rename dest to add ".bak" extension (clobbering any existing backup), then
    rename src to dest.
    '''
    os.replace(dest, f'{dest}.bak')
    rename_noreplace(src, dest)

def rename_with_backup(src, dest):
    '''Rename dest to add unique extension, then rename src to dest.'''
    def utc_timestamp_sfx():
        return datetime.datetime.now(datetime.timezone.utc).strftime('%Y%m%d%H%M%S')
    # NOTE: Using PID in backup file's name is for informational purposes only,
    #        to aid in debugging to figure out which current or past process
    #        created a particular backup file.
    pid = os.getpid()
    while not try_rename_noreplace(dest,
                                   f'{dest}.bak.{utc_timestamp_sfx()}.{pid}'):
        pass
    rename_noreplace(src, dest)

###############################################################################
## CONVENTIONAL SAFE FILE-HANDLING UTILITY FUNCTIONS: #########################
## (only safe if all relevant parties follow same convention) #################

class ClaimedFile(LoggerMixin):

    def __init__(self, file_ctxmgr=None, *, lock_file_name=None):
        if callable(getattr(file_ctxmgr, 'filename', None)):
            self.__lock_file_name = f'{file_ctxmgr.filename()}.lck'
        else:
            if lock_file_name is None:
                raise RuntimeError('Cannot determine lock file name because'
                                   ' target file CM lacks a \'filename\''
                                   ' attribute and lock file name was not'
                                   ' specified.')
            self.__lock_file_name = lock_file_name
        LoggerMixin.__init__(self, f'"{self.__lock_file_name}"')
        self.__lock_file_ctxmgr = None
        self.__file_ctxmgr = file_ctxmgr
        self.__pid = os.getpid()

    def __enter__(self):
        self._logger().debug('Opening lock file for exclusive creation and'
                             ' writing.')
        self.__lock_file_ctxmgr = open(self.__lock_file_name,
                                       'x',
                                       encoding='UTF-8')
        self._logger().debug('Entering lock file CM.')
        lock_file = self.__lock_file_ctxmgr.__enter__()
        # NOTE: Writing PID is for informational purposes only.  It won't be
        #        programatically verified by anything afterwards, and if all
        #        parties that could potentially access a given file are using
        #        this exact mechanism, it shouldn't be necessary to verify.
        #        The PID is just there to aid in debugging to figure out which
        #        current or past process created a particular lock file, for
        #        instance if a program is long-running and still has a file
        #        locked, or if a program crashes before removing a lock file.
        print(f'Locked by PID {self.__pid}.', file=lock_file, flush=True)
        if self.__file_ctxmgr is None:
            return None
        self._logger().debug('Entering target file CM.')
        return self.__file_ctxmgr.__enter__()

    def __exit__(self, *exc_details):
        exception_was_raised = any(x is not None for x in exc_details)
        exc = exc_details[1]
        if exception_was_raised:
            self._logger().debug(f'Exiting with exception: {repr(exc)}')
        else:
            self._logger().debug('Exiting normally.')

        if self.__lock_file_ctxmgr:
            # If lock file's context manager (first bit of state initialized in
            #  __init__()) is set, that means we have some cleaning up to do.

            suppress_exception = False

            # First, clean up target file's context manager (reverse of
            #  initialization order) if there was one (caller may have used
            #  this class just to manage a lock file and not a target file).
            if self.__file_ctxmgr is not None:
                self._logger().debug('Exiting target file CM.')
                if self.__file_ctxmgr.__exit__(*exc_details):
                    self._logger().debug('Exiting target file CM suppressed'
                                         ' exception.')
                    suppress_exception = True
                    exc_details = (None, None, None)
                self.__file_ctxmgr = None

            # Second, clean up lock file's context manager (reverse of
            #  initialization order).
            self._logger().debug('Exiting lock file CM.')
            if self.__lock_file_ctxmgr.__exit__(*exc_details):
                self._logger().debug('Exiting lock file CM suppressed'
                                     ' exception.')
                suppress_exception = True
                exc_details = (None, None, None)
            self.__lock_file_ctxmgr = None

            # Finally, clean up lock file itself.
            self._logger().debug('Deleting lock file.')
            try:
                os.remove(self.__lock_file_name)
            except Exception as e:
                self._logger().error(f'Failed to delete lock file: {repr(e)}')

            self._logger().debug(('Done, will suppress exception.'
                                  if suppress_exception else
                                  'Done, will allow exception to propagate.')
                                 if exception_was_raised else
                                 'Done.')
            return exception_was_raised and suppress_exception

        # If lock file's context manager (first bit of state initialized in
        #  __init__()) wasn't set, that means an exception was raised from
        #  __init__(), or from __enter__() while lock file was being opened.
        self._logger().debug('Done, will allow exception to propagate.')
        assert exception_was_raised
        return False

class BaseWriteFile(abc.ABC, LoggerMixin):

    @staticmethod
    def __valid_filename(filename):
        if len(filename) == 0:
            raise ValueError('filename cannot be blank')
        if os.path.basename(filename) == '':
            raise ValueError('filename cannot refer to a directory (end with'
                            f' {filename[-1]}): "{filename}"')
        return filename

    def __init__(self,
                 filename,
                 *,
                 binary=False,
             # params passed through to `open()` for target/proxy file(s):
                 buffering=-1,
                 encoding=None,
                 errors=None,
                 newline=None):
        LoggerMixin.__init__(self, f'"{filename}"')
        self._filename = self.__valid_filename(filename)
        self._binary_mode = 'b' if binary else ''
        self._open_kwargs = {'buffering': buffering,
                             'encoding':  encoding,
                             'errors':    errors,
                             'newline':   newline}

    def filename(self):
        return self._filename

    @abc.abstractmethod
    def __enter__(self):
        pass

    @abc.abstractmethod
    def __exit__(self, *exc_details):
        pass

class WriteInPlaceFile(BaseWriteFile):

    def __init__(self,
                 filename,
                 *,
                 must_already_exist=False,
                 overwrite=False,
                 **open_kwargs):
        super().__init__(filename, **open_kwargs)
        self.__must_already_exist = must_already_exist
        self.__overwrite = overwrite
        self.__file_ctxmgr = None

    def __enter__(self):
        try:
            # Attempt to open target file in read+write mode.
            # NOTE: You could use the lower level `os.open()` in order to make
            #        use of lower level file modes like `os.O_RDWR|os.O_TRUNC`,
            #        but then the returned object would be a file descriptor,
            #        not a context manager.  So instead you use `open()` below
            #        with 'r+' for read+write mode (since that's the only
            #        write-capable mode you can pass to `open()` that will
            #        require the file to exist) and manually handle the
            #        resetting of position and truncation yourself.
            self._logger().debug('Opening target file for reading and'
                                 ' writing.')
            self.__file_ctxmgr = open(self._filename,
                                      f'r+{self._binary_mode}',
                                      **self._open_kwargs)
            self._logger().debug('Entering target file CM.')
            file = self.__file_ctxmgr.__enter__()
            if self.__overwrite:
                self._logger().debug('Truncating existing target file to 0'
                                     ' bytes.')
                file.truncate()
            if self.__must_already_exist:
                # If caller required target file to already exist (and it did,
                #  b/c otherwise FileNotFoundError would've been raised), then
                #  there's no point in returning bool of whether target file
                #  already existed b/c it'll always be true, so return just the
                #  target file object.
                return file
            return file, True # True: target file already existed
        except FileNotFoundError:
            if self.__must_already_exist:
                # If caller required target file to already exist (and it
                #  didn't, b/c FileNotFoundError was raised), then re-raise
                #  exception.
                raise
            # Target file didn't already exist, so create it anew and open that
            #  instead.
            self._logger().debug('Target file didn\'t exist, so instead'
                                 ' opening it for exclusive creation and'
                                 ' writing.')
            self.__file_ctxmgr = open(self._filename,
                                      f'x{self._binary_mode}',
                                      **self._open_kwargs)
            self._logger().debug('Entering target file CM.')
            return (self.__file_ctxmgr.__enter__(),
                    False) # False: target file didn't already exist

    def __exit__(self, *exc_details):
        exception_was_raised = any(x is not None for x in exc_details)
        exc = exc_details[1]
        if exception_was_raised:
            self._logger().debug(f'Exiting with exception: {repr(exc)}')
        else:
            self._logger().debug('Exiting normally.')

        if self.__file_ctxmgr:
            # If target file's context manager (first bit of state initialized
            #  in __init__()) is set, that means we have some cleaning up to
            #  do.

            suppress_exception = False

            # Clean up target file's context manager.
            self._logger().debug('Exiting target file CM.')
            if self.__file_ctxmgr.__exit__(*exc_details):
                self._logger().debug('Exiting target file CM suppressed'
                                     ' exception.')
                suppress_exception = True
                exc_details = (None, None, None)
            self.__file_ctxmgr = None

            self._logger().debug(('Done, will suppress exception.'
                                  if suppress_exception else
                                  'Done, will allow exception to propagate.')
                                 if exception_was_raised else
                                 'Done.')
            return exception_was_raised and suppress_exception

        # If target file's context manager (first bit of state initialized in
        #  __init__()) wasn't set, that means an exception was raised from
        #  __init__(), or from __enter__() while target file was being opened.
        self._logger().debug('Done, will allow exception to propagate.')
        assert exception_was_raised
        return False

class AbortProxyWrite(Exception):
    def __init__(self, delete_write_proxy_file=True):
        self.__delete_write_proxy_file = delete_write_proxy_file
    def should_delete_write_proxy_file(self):
        return self.__delete_write_proxy_file

class WriteProxiedFile(BaseWriteFile):

    @staticmethod
    def __valid_num_backups(num_backups):
        VALID_NUM_BACKUPS_VALUES = (0, 1, float('inf'))
        if num_backups not in VALID_NUM_BACKUPS_VALUES:
            raise ValueError('num_backups must be one of:'
                            f' {", ".join(VALID_NUM_BACKUPS_VALUES)}')
        return num_backups

    def __init__(self,
                 filename,
                 *,
                 must_already_exist=False,
                 num_backups=0, # supports 0, 1, and inf (i.e. `float('inf')` or `math.inf`)
                 delete_write_proxy_file_on_failure=True,
                 **open_kwargs):
        super().__init__(filename, **open_kwargs)
        self.__must_already_exist = must_already_exist
        self.__num_backups = self.__valid_num_backups(num_backups)
        self.__delete_write_proxy_file_on_failure = delete_write_proxy_file_on_failure
        self.__write_proxy_file_ctxmgr = None
        self.__file_ctxmgr = None

    def __enter__(self):
        # Open write-proxy file in exclusive-create mode.
        self._logger().debug('Opening write-proxy file for exclusive creation'
                             ' and writing.')
        self.__write_proxy_file_ctxmgr = open(f'{self._filename}.tmp',
                                              f'x{self._binary_mode}',
                                              **self._open_kwargs)
        try:
            # Attempt to open target file in read-only mode.
            self._logger().debug('Opening target file for reading.')
            self.__file_ctxmgr = open(f'{self._filename}',
                                      f'r{self._binary_mode}',
                                      **self._open_kwargs)
            self._logger().debug('Entering target file CM and write-proxy file'
                                 ' CM.')
            return (self.__file_ctxmgr.__enter__(),
                    self.__write_proxy_file_ctxmgr.__enter__())
        except FileNotFoundError:
            if self.__must_already_exist:
                # Caller required target file to already exist, so re-raise the
                #  FileNotFoundError.
                raise
            # Caller didn't require target file to already exist, so just leave
            #  its context manager as None, and return None instead of the
            #  opened target file object.
            self._logger().debug('Target file didn\'t exist.')
            self.__file_ctxmgr = None
            self._logger().debug('Entering write-proxy file CM.')
            return None, self.__write_proxy_file_ctxmgr.__enter__()

    def __exit__(self, *exc_details):
        exception_was_raised = any(x is not None for x in exc_details)
        exc = exc_details[1]
        if exception_was_raised:
            self._logger().debug(f'Exiting with exception: {repr(exc)}')
        else:
            self._logger().debug('Exiting normally.')

        if self.__write_proxy_file_ctxmgr:
            # If write-proxy file's context manager (first bit of state
            #  initialized in __init__()) is set, that means we have some
            #  cleaning up to do.

            suppress_exception = False

            # First, clean up target file's context manager (reverse of
            #  initialization order).
            if self.__file_ctxmgr:
                # If target file's context manager is set, clean it up.
                file_already_existed = True
                self._logger().debug('Exiting target file CM.')
                if self.__file_ctxmgr.__exit__(*exc_details):
                    self._logger().debug('Exiting target file CM suppressed'
                                         ' exception.')
                    suppress_exception = True
                    exc_details = (None, None, None)
                self.__file_ctxmgr = None
            else:
                # If target file's context manager wasn't set, and given the
                #  other conditions above, that means either an exception was
                #  raised from __enter__() while target file was being opened,
                #  or target file didn't already exist.
                if not exception_was_raised:
                    # If no exception was raised, and given the other
                    #  conditions above, then target file must not have already
                    #  existed, and therefore also caller must not have
                    #  required it to already exist.
                    file_already_existed = False
                    assert not self.__must_already_exist
                # NOTE: If exception_was_raised, we deliberately don't declare
                #        file_already_existed because we don't know whether the
                #        target file already existed, but that's fine because
                #        we don't check file_already_existed below if
                #        exception_was_raised.

            # Second, clean up write-proxy file's context manager (reverse of
            #  initialization order).
            self._logger().debug('Exiting write-proxy file CM.')
            if self.__write_proxy_file_ctxmgr.__exit__(*exc_details):
                self._logger().debug('Exiting write-proxy file CM suppressed'
                                     ' exception.')
                suppress_exception = True
                exc_details = (None, None, None)
            self.__write_proxy_file_ctxmgr = None

            # Finally, reconcile write-proxy file with target file.
            if exception_was_raised:
                # Something went wrong (exception was raised), or caller just
                #  decided to abort creating/replacing target file
                #  (AbortProxyWrite was raised).  Discard write-proxy file
                #  unless instructed otherwise.
                if isinstance(exc, AbortProxyWrite):
                    suppress_exception = True
                    delete_write_proxy_file = exc.should_delete_write_proxy_file()
                else:
                    delete_write_proxy_file = self.__delete_write_proxy_file_on_failure
                if delete_write_proxy_file:
                    self._logger().debug('Deleting write-proxy file.')
                    try:
                        os.remove(f'{self._filename}.tmp')
                    except Exception as e:
                        self._logger().error('Failed to delete write-proxy'
                                            f' file: {repr(e)}')
            else:
                # Writes completed successfully, so follow through on whatever
                #  strategy was requested.
                if file_already_existed:
                    # If target file already existed, then we should either:
                    #  * rename target file to a backup name, then rename
                    #     write-proxy file to target file (and expect to not be
                    #     replacing an existing target file)
                    #  * replace target file with write-proxy file
                    self._logger().debug('Replacing target file with'
                                         ' write-proxy file, keeping'
                                        f' {self.__num_backups} backup(s).')
                    if self.__num_backups == float('inf'):
                        rename_with_backup(f'{self._filename}.tmp',
                                           self._filename)
                    elif self.__num_backups == 1:
                        rename_with_single_backup(f'{self._filename}.tmp',
                                                  self._filename)
                    else:
                        assert self.__num_backups == 0
                        os.replace(f'{self._filename}.tmp', self._filename)
                        # NOTE: `os.replace()` will clobber dest (if necessary)
                        #        on all platforms, whereas `os.rename()` only
                        #        does so on POSIX platforms.
                else:
                    # If target file didn't already exist, then we should
                    #  rename write-proxy file to target file (and expect to
                    #  not be replacing an existing target file).
                    self._logger().debug('Renaming write-proxy file to target'
                                         ' file.')
                    rename_noreplace(f'{self._filename}.tmp', self._filename)

            self._logger().debug(('Done, will suppress exception.'
                                  if suppress_exception else
                                  'Done, will allow exception to propagate.')
                                 if exception_was_raised else
                                 'Done.')
            return exception_was_raised and suppress_exception

        # If write-proxy file's context manager (first bit of state initialized
        #  in __init__()) wasn't set, that means an exception was raised from
        #  __init__(), or from __enter__() while write-proxy file was being
        #  opened.
        self._logger().debug('Done, will allow exception to propagate.')
        assert exception_was_raised
        return False

###############################################################################
## MISC UTILITIES: ############################################################

class RedirectStdIO:

    def __init__(self, std_fd, target_file_path, flags=None):

        # validate std_fd, then do some init
        if callable(getattr(std_fd, 'fileno', None)):
            std_fd = std_fd.fileno()
        if std_fd not in (0, 1, 2):
            raise ValueError('std_fd must be stdin, stdout, or stderr')
        self.__std_fd = std_fd

        # do some init
        self.__target_file_path = target_file_path

        # validate flags, then do some init
        if flags is None:
            if self.__std_fd == 0:
                self.__flags = os.O_RDONLY
            else:
                self.__flags = os.O_WRONLY
                if self.__target_file_path != os.devnull:
                    self.__flags |= os.O_APPEND | os.O_CREAT
        else:
            # NOTE: `man open` explains that unlike the other flags, O_RDONLY,
            #        O_WRONLY, and O_RDWR are not actually flags, but in fact
            #        each one is a value for the entire low order 2 bits. So,
            #        they cannot be checked for by simply masking `flags` like
            #        `if flags & os.O_RDONLY`, those low order bits must be
            #        extracted and compared directly to `O_RDONLY`, etc. for
            #        equality.
            flags_loworder_bits = flags & 0b11
            if self.__std_fd == 0:
                if flags_loworder_bits not in (os.O_RDONLY, os.O_RDWR):
                    raise ValueError('flags must be O_RDONLY or O_RDWR when'
                                     ' std_fd is stdin')
            else:
                if flags_loworder_bits not in (os.O_WRONLY, os.O_RDWR):
                    raise ValueError('flags must be O_WRONLY or O_RDWR when'
                                     ' std_fd is stdout or stderr')
            self.__flags = flags

        # do some init
        self.__target_file_fd = None
        self.__std_backup_fd = None

    def __enter__(self):

        # open target file which we'll be redirecting std fd to/from, i.e. get
        #  fd pointing to file description of opened target file
        self.__target_file_fd = os.open(self.__target_file_path, self.__flags)

        # create new "backup" fd pointing to same file description that std fd
        #  is pointing to
        self.__std_backup_fd = os.dup(self.__std_fd)

        # redirect std fd to point to file description of opened target file
        os.dup2(self.__target_file_fd, self.__std_fd)

        return None

    def __exit__(self, *exc_details):
        if self.__std_backup_fd is not None:

            # redirect std fd to point back to file description it had pointed
            #  to originally
            os.dup2(self.__std_backup_fd, self.__std_fd)

            # close backup fd, but since std fd now points to same file
            #  description, it won't actually close file, just removes backup
            #  fd from fd table
            os.close(self.__std_backup_fd)

        if self.__target_file_fd is not None:

            # close target file fd, and since now no other fd points to target
            #  file description, it will actually close target file in addition
            #  to removing target file fd from fd table
            os.close(self.__target_file_fd)

        return False

# Sets up handling of control signals that Python doesn't normally handle, and
#  makes it so that they interrupt this program's main thread with SIGINT.
#  Caller just needs to ensure main thread handles SIGINT by gracefully exiting
#  the program, and doing so quickly (since Windows will forcefully terminate
#  the program upon receipt of certain control signals, if program does not
#  exit of its own accord in a timely fashion).
def windows_setup_graceful_exit_for_python_unhandled_control_signals():
    logger = logging.getLogger(
                     'windows_setup_graceful_exit_for_python_unhandled_control_signals')

    # NOTE: References for this function's code:
    #        Example from within CPython itself:
    #         https://github.com/python/cpython/blob/3.8/Lib/test/win_console_handler.py
    #        Example from a bug report (better than above):
    #         https://github.com/python/cpython/issues/85470
    #        CTRL_*_EVENT values, timeouts, and handler behavior:
    #         https://learn.microsoft.com/en-us/windows/console/handlerroutine
    #         https://stackoverflow.com/questions/3640633/setconsolectrlhandler-routine-issue
    #        SetConsoleCtrlHandler:
    #         https://learn.microsoft.com/en-us/windows/console/setconsolectrlhandler

    import ctypes
    import ctypes.wintypes
    import enum
    import threading

    # Define enum class to represent Windows control signals.
    class WindowsControlSignal(enum.IntEnum):
        # NOTE: The signal module defines its own CTRL_C_EVENT and
        #        CTRL_BREAK_EVENT (with same values as below) but not other
        #        CTRL_*_EVENT values, so we just define all of them without
        #        using the signal module.
        CTRL_C_EVENT        =  0, True          # Ctrl+C
        CTRL_BREAK_EVENT    =  1, False         # Ctrl+Break
        CTRL_CLOSE_EVENT    =  2, False, 5000   # close console window
        CTRL_LOGOFF_EVENT   =  5, False, 500    # logoff
        CTRL_SHUTDOWN_EVENT =  6, False, 500    # shutdown
        UNKNOWN_CTRL_SIGNAL = -1, False, 500
        def __new__(cls,
                    value,
                    python_should_handle,
                    unavoidable_termination_msec=None):
            obj = int.__new__(cls)
            obj._value_ = value
            obj.python_should_handle = python_should_handle
            obj.unavoidable_termination_msec = unavoidable_termination_msec
            return obj

    # Define wrapper function for interrupting this program's main thread.
    def interrupt_main_thread():
        # Signals always get handled by the main thread in Python.  SIGINT is
        #  handled by Python on all OSes, by raising a KeyboardInterrupt
        #  exception, but it does not really support other signals well on
        #  Windows, so we just use SIGINT.  See:
        #   https://github.com/python/cpython/issues/70538
        #   https://stackoverflow.com/questions/47306805/signal-sigterm-not-received-by-subprocess-on-windows
        signal.raise_signal(signal.SIGINT)

    # Define control signal handler function.
    # NOTE: The handler is registered via a C interface so it must be global,
    #        otherwise it will be in this function's scope, and since there is
    #        no in-Python reference to it outside of this function, it may get
    #        garbage-collected before Windows can call it.
    global windows_console_ctrl_handler
    @ctypes.WINFUNCTYPE(ctypes.wintypes.BOOL, ctypes.wintypes.DWORD)
    def windows_console_ctrl_handler(ctrl_signal):
        # NOTE: This runs in new thread that is created by Windows itself, see:
        #        https://github.com/python/cpython/issues/77426#issuecomment-1093781048

        # init stuff
        logger = logging.getLogger('windows_console_ctrl_handler')
        try:
            ctrl_signal = WindowsControlSignal(ctrl_signal)
            ctrl_signal_display = f'{ctrl_signal.name} ({ctrl_signal.value})'
        except ValueError:
            ctrl_signal_display = f'unknown control signal ({ctrl_signal})'
            ctrl_signal = WindowsControlSignal.UNKNOWN_CTRL_SIGNAL
            logger.error(f'Received {ctrl_signal_display}, will treat it as'
                          ' signal that Python won\'t handle and which has'
                          ' unavoidable termination by Windows.')

        # don't handle signals here that Python should handle
        if ctrl_signal.python_should_handle:
            logger.debug(f'Ignoring {ctrl_signal_display} here, since it'
                          ' should be handled by Python.')
            return False # signal not handled here (let it propagate to other
                         #  registered handlers, like Python's built-in one)

        # handle signals that Python won't handle and which have unavoidable
        #  termination, by interrupting main thread (giving it a chance to
        #  cleanup), while in this thread waiting for main thread to exit
        if ctrl_signal.unavoidable_termination_msec is not None:
            logger.debug(f'Received {ctrl_signal_display}, being handled in'
                          ' new thread. Windows will forcefully terminate'
                          ' program in as little as'
                         f' {ctrl_signal.unavoidable_termination_msec} msec if'
                          ' it does not exit on its own by then. Will now'
                          ' interrupt main thread to induce graceful exit,'
                          ' which will hopefully happen within available'
                          ' time.')
            interrupt_main_thread()
            logger.debug('Interrupted main thread. Windows will immediately'
                         ' forcefully terminate program once this handler'
                         ' returns, so will now keep sleeping for as long as'
                         ' main thread is alive, to allow any cleanup actions'
                         ' to complete.')
            while threading.main_thread().is_alive():
                time.sleep(1)
            # program should've been terminated by here, so code below
            #  shouldn't matter
            logger.debug('Main thread ended. Done handling control signal.')
            return False # signal can't really be handled (returning True won't
                         #  prevent propagation of signals that have
                         #  unavoidable termination, so might as well return
                         #  False since it'll anyway behave as if we did)

        # handle signals that Python won't handle (and which have *avoidable*
        #  termination) by interrupting main thread (giving it a chance to
        #  cleanup)
        logger.debug(f'Received {ctrl_signal_display}, being handled in new'
                      ' thread. Will now interrupt main thread to induce'
                      ' graceful exit.')
        interrupt_main_thread()
        logger.debug('Interrupted main thread. This handler will now return.'
                     ' Done handling control signal.')
        return True # signal was handled (don't let it propagate)

    # The predefined kernel32 accessor at ctypes.windll.kernel32 doesn't have
    #  use_last_error enabled, so we must create our own accessor instance to
    #  enable it.
    kernel32 = ctypes.WinDLL('kernel32', use_last_error=True)

    # Finally, we register the above handler.
    add_handler = True
    if not kernel32.SetConsoleCtrlHandler(windows_console_ctrl_handler,
                                          add_handler):
        print('\n'
              'DO NOT CLOSE THIS CONSOLE WINDOW, OR TYPE CTRL+BREAK, OR'
              ' LOGOFF/SHUTDOWN WHILE THIS IS RUNNING!\n'
              'Failed to register handler for these events, so if they occur,'
             f' then cleanup will not happen (see "{LOG_FILE_NAME}" for'
              ' details).\n'
              'To abort gracefully, type Ctrl+C.\n'
              '\n',
              end='',
              file=sys.stderr)
        logger.error('Failed to register control signal handler:'
                    f' {ctypes.WinError(ctypes.get_last_error())}')
        return
    if sys.version_info >= (3, 9):
        print('\n'
              'DO NOT LOGOFF/SHUTDOWN WHILE THIS IS RUNNING!\n'
              'Python 3.9+ has a bug that prevents handling of these events,'
              ' so if they occur, then cleanup will not happen (see'
             f'"{LOG_FILE_NAME}" for details).\n'
              '\n',
              end='',
              file=sys.stderr)
        logger.warning('Python 3.9+ has bug that prevents handling of Windows'
                       ' logoff/shutdown events:'
                       ' https://github.com/python/cpython/issues/85470')

###############################################################################
## KEYPRESS DETECTION UTILITIES: ##############################################

def __get_interruptible_getch_from_tty_for_Windows():
    logger = logging.getLogger('__get_interruptible_getch_for_Windows')
    logger.info('Initializing interruptible getch for Windows.')
    import msvcrt
    def interruptible_getch_from_tty__closure():
        # wait until keypress
        timeout_secs = 0.1
        while not msvcrt.kbhit():
            time.sleep(timeout_secs)

        # get keypress, return result
        #  (msvcrt doc says the 2 bytes below indicate there's 1 more byte)
        ch = msvcrt.getch()
        if ch in (b'\x00', b'\xe0'):
            return ch + msvcrt.getch()
        return ch
    logger.info('Interruptible getch implemented using msvcrt module (Windows'
                ' only).')
    return interruptible_getch_from_tty__closure

def __get_interruptible_getch_from_tty_for_POSIX():
    logger = logging.getLogger('__get_interruptible_getch_for_POSIX')
    logger.info('Initializing interruptible getch for POSIX.')
    import select
    import termios
    import tty
    class UnbufferedNonEchoConsole:
        def __enter__(self):
            self.__old_attr = termios.tcgetattr(sys.stdin)
            tty.setcbreak(sys.stdin.fileno())
            return self
        def __exit__(self, *exc_details):
            termios.tcsetattr(sys.stdin, termios.TCSAFLUSH, self.__old_attr)
    def interruptible_getch_from_tty__closure():
        with UnbufferedNonEchoConsole():
            # wait until keypress
            timeout_secs = 0.1
            while not select.select([sys.stdin], [], [], timeout_secs)[0]:
                pass

            # get keypress, return result
            ch = b''
            while select.select([sys.stdin], [], [], 0.0001)[0]:
                # must use os.read, sys.stdin.read is higher level and will
                #  read more than requested num bytes to fill its internal
                #  buffer, thus throwing off the select call
                ch += os.read(sys.stdin.fileno(), 1)
        return ch
    logger.info('Interruptible getch implemented using select.select with'
                ' stdin (POSIX only).')
    return interruptible_getch_from_tty__closure

interruptible_getch_from_tty = {
    'Windows': __get_interruptible_getch_from_tty_for_Windows
}.get(platform.system(), __get_interruptible_getch_from_tty_for_POSIX)()

class NonTTYException(Exception):
    pass

def confirmation_prompt(prompt,
                        *,
                        negative_feedback=False,
                        positive_feedback=True):
    '''Prompt user for confirmation by single keypress of Y or N.

    Arguments:
        prompt (str)                    Text to display to prompt user for
                                         input. On invalid keypress, prompt
                                         will be repeated.
        negative_feedback (bool or str) If False (or empty str), do not echo
                                         invalid keypresses.
                                        If True or a non-empty str, echo
                                         invalid keypress, and if a non-empty
                                         str then print that str before
                                         repeating prompt.
        positive_feedback (bool)        If False, do not echo valid keypress.
                                        If True, echo valid keypress.

    Raises:
        NonTTYException     stdin or stdout is not a TTY. Text of exception
                             will specify which.

    Returns:
        bool    Whether user confirmed.
    '''
    if not os.isatty(sys.stdin.fileno()):
        raise NonTTYException('stdin is not a TTY')
    if not os.isatty(sys.stdout.fileno()):
        raise NonTTYException('stdout is not a TTY')
    print(prompt, end='', flush=True)
    yes = (b'y', b'Y')
    no  = (b'n', b'N')
    if negative_feedback:
        printable = set(string.printable.encode())
    while True:
        # must use something interruptible, particularly on Windows, so that
        #  signals can interrupt main thread
        ch = interruptible_getch_from_tty()
        if ch in yes:
            confirmed = True
            break
        if ch in no:
            confirmed = False
            break
        if negative_feedback:
            if len(ch) == 1 and ord(ch) in printable:
                os.write(sys.stdout.fileno(), ch)
            print()
            if isinstance(negative_feedback, str):
                print(negative_feedback)
            print(prompt, end='', flush=True)
    if positive_feedback:
        os.write(sys.stdout.fileno(), ch)
    print()
    return confirmed

###############################################################################
## ASYNCIO STREAMREADER UTILITIES: ############################################

class IncompleteDataException(Exception):
    pass

# NOTE: When this function is called, it just returns a coroutine object, it
#        doesn't run the code you see below.
async def streamreader_to_json_objs(sr,
                                    obj_cb,
                                    *,
                                    encoding='UTF-8',
                                    max_chunk_size=4096):
    '''Decodes StreamReader bytes to text, then text to JSON objects.

    Arguments:
        sr             (asyncio.StreamReader)   Interface to read from an I/O
                                                 stream asynchronously.
        obj_cb         (function)               Function to be called for each
                                                 decoded JSON object. If it
                                                 returns False, this function
                                                 will stop reading from
                                                 StreamReader and also return
                                                 False.
        encoding       (str)                    Text encoding to use when
                                                 decoding StreamReader bytes
                                                 (before decoding to JSON).
        max_chunk_size (int)                    Maximum number of bytes to read
                                                 at a time from StreamReader.
                                                 This is purely a performance
                                                 tunable, it should not affect
                                                 the results as the logic can
                                                 handle multibyte characters
                                                 or JSON objects that span
                                                 multiple chunks.

    Raises:
        IncompleteDataException     I/O stream contained partial data that
                                     could not be decoded to text, or partial
                                     text that could not be decoded to JSON.

    Returns:
        True    if all StreamReader content consumed
        False   if returning before all StreamReader content is consumed (i.e.
                 obj_cb returned False)
    '''

    # stateful Unicode decoder capable of partial decodes
    unicode_dec = codecs.getincrementaldecoder(encoding)()

    # stateless JSON decoder capable of partial decodes
    #  (and separate variable to hold the state)
    json_dec = json.JSONDecoder()
    pending_json_data = ''

    while True:

        # read bytes chunk
        chunk = await sr.read(max_chunk_size)
        if not chunk:
            if unicode_dec.getstate()[0]:
                raise IncompleteDataException(
                              f'A partial {encoding} byte sequence remains,'
                               ' but there is no more data left to complete'
                              f' it: {unicode_dec.getstate()[0]}')
            if pending_json_data:
                raise IncompleteDataException(
                               'A partial JSON document remains, but there is'
                               ' no more data left to complete it:'
                              f' {pending_json_data}')
            break

        # decode bytes chunk to specified encoding (should be some kind of
        #  Unicode) and accumulate it as pending JSON data
        if pending_json_data:
            # There is a partial JSON document pending from a previous
            #  iteration, so just append the decoded chunk to it as-is.
            pending_json_data += unicode_dec.decode(chunk)
        else:
            # There is no pending JSON data, so we should lstrip the next bytes
            #  we read since there may be whitespace between JSON documents.
            pending_json_data = unicode_dec.decode(chunk).lstrip()

        # decode each whole JSON document within pending JSON data to an object
        while pending_json_data:
            try:
                obj, num_bytes_read = json_dec.raw_decode(pending_json_data)
            except json.JSONDecodeError:
                break
            if not obj_cb(obj):
                return False
            pending_json_data = pending_json_data[num_bytes_read:].lstrip()

    return True

# NOTE: When this function is called, it just returns a coroutine object, it
#        doesn't run the code you see below.
async def streamreader_to_lines(sr, line_cb, *, encoding='UTF-8'):
    '''Decodes StreamReader bytes to lines of text.

    Arguments:
        sr       (asyncio.StreamReader) Interface to read from an I/O stream
                                         asynchronously.
        line_cb  (function)             Function to be called for each decoded
                                         line.
        encoding (str)                  Text encoding to use when decoding
                                         StreamReader bytes.
    '''
    while True:
        buf = await sr.readline()
        if not buf:
            break
        line = buf.decode(encoding).rstrip()
        line_cb(line)

###############################################################################
## BUSINESS LOGIC: ############################################################

def read_config(config_file):
    config = json.load(config_file)
    duplicate_urls = {playlist_url
                      for playlist_url, count
                       in collections.Counter(
                                  playlist_info['url']
                                  for playlist_info
                                   in config['playlist_infos']).items()
                       if count > 1}
    if len(duplicate_urls) > 0:
        raise RuntimeError('Duplicate `url` values found in config file'
                          f' "{os.path.basename(config_file.name)}":'
                          f' {", ".join(duplicate_urls)}')
    return config

def read_state(state_file):
    state = {} if state_file is None else json.load(state_file)

    # read video IDs acknowledged by user before last report
    video_ids_acked_before_last_report = None
    if 'pre-report' in state:
        video_ids_acked_before_last_report = {}
        duplicate_urls = set()
        for playlist_pre_report_state in state['pre-report']:
            playlist_url = playlist_pre_report_state['playlist_url']
            if playlist_url in video_ids_acked_before_last_report:
                duplicate_urls.add(playlist_url)
                continue
            video_ids_acked_before_last_report[playlist_url] \
                    = set(playlist_pre_report_state['acked_video_ids']
                                  .split(','))
        if len(duplicate_urls) > 0:
            raise RuntimeError('Duplicate `playlist_url` values found under'
                               ' "pre-report" in state file'
                              f' "{os.path.basename(state_file.name)}":'
                              f' {", ".join(duplicate_urls)}')

    # read data that went into last report
    last_report_data = None
    if 'report' in state:
        last_report_data = {}
        duplicate_urls = set()
        for playlist_report_state in state['report']:
            playlist_url = playlist_report_state['playlist_url']
            if playlist_url in last_report_data:
                duplicate_urls.add(playlist_url)
                continue
            playlist_report_data = last_report_data.setdefault(playlist_url,
                                                               {})
            playlist_report_data['playlist_name'] \
                    = playlist_report_state['playlist_name']
            if 'notices' in playlist_report_state:
                playlist_report_data['notices'] \
                        = playlist_report_state['notices']
            if 'videos_older_to_newer' in playlist_report_state:
                playlist_report_data['videos_older_to_newer'] \
                        = playlist_report_state['videos_older_to_newer']
        if len(duplicate_urls) > 0:
            raise RuntimeError('Duplicate `playlist_url` values found under'
                               ' "report" in state file'
                              f' "{os.path.basename(state_file.name)}":'
                              f' {", ".join(duplicate_urls)}')

    return video_ids_acked_before_last_report, last_report_data

def write_config(config_file, config):
    json.dump(config,
              config_file,
              ensure_ascii=False, # don't escape Unicode chars, leave them as-is
              indent=4)
    print(file=config_file) # json.dump doesn't print final line terminator

def write_state(state_file, acked_video_ids, report_data):
    def gen_playlist_report_state(playlist_url, playlist_report_data):
        playlist_report_state \
                = {'playlist_url':  playlist_url,
                   'playlist_name': playlist_report_data['playlist_name']}
        for field in ('notices', 'videos_older_to_newer'):
            if field in playlist_report_data:
                playlist_report_state[field] = playlist_report_data[field]
        return playlist_report_state
    pre_report_state = [{'playlist_url':    playlist_url,
                         'acked_video_ids': ','.join(sorted(playlist_acked_video_ids))}
                        for playlist_url, playlist_acked_video_ids
                         in acked_video_ids.items()]
    report_state = [gen_playlist_report_state(playlist_url,
                                              playlist_report_data)
                    for playlist_url, playlist_report_data
                     in report_data.items()]
    state = {}
    if pre_report_state:
        state['pre-report'] = pre_report_state
    if report_state:
        state['report'] = report_state
    json.dump(state,
              state_file,
              ensure_ascii=False, # don't escape Unicode chars, leave them as-is
              indent=4)
    print(file=state_file) # json.dump doesn't print final line terminator

# NOTE: When this function is called, it just returns a coroutine object, it
#        doesn't run the code you see below.
async def get_playlist_report_data__async(playlist_report_data, # input/output param
                                          playlist_acked_video_ids,
                                          playlist_url,
                                          playlist_name=None):
    logger = logging.getLogger('get_playlist_report_data__async')

    # init playlist display text
    if playlist_name is None:
        playlist_display_text = f'{playlist_url}'
    else:
        playlist_display_text = f'{playlist_url} ("{playlist_name}")'

    # init list of already-known video IDs, composed of the following:
    #  * list of videos user has "acknowledged" by having previously deleted
    #     report files which contained them
    #  * if there currently is a report file, list of videos in it
    already_known_video_ids = copy.deepcopy(playlist_acked_video_ids)
    if 'videos_older_to_newer' in playlist_report_data:
        already_known_video_ids \
                |= {v['id']
                    for v in playlist_report_data['videos_older_to_newer']}

    # init command to download playlist
    cmd = ['yt-dlp', '--ignore-config',
                     '--no-cache-dir',
                     '--verbose',

                     '--encoding=UTF-8',
                        # not documented very well, but is needed at least on
                        #  Windows, where stderr output of yt-dlp may contain
                        #  non-UTF-8 encoding otherwise (causing decoding
                        #  errors later, since we assume UTF-8)
                        # NOTE: manpage says this option is experimental. If it
                        #        is ever removed, alternative fix is to be more
                        #        lenient when decoding stderr.

                     '--flat-playlist',
                        # not documented very well, but it seems this will
                        #  internally download only playlist pages, not each
                        #  video's page, which should be faster but will also
                        #  limit what metadata is available per video

                     '--lazy-playlist',
                        # print requested info about each playlist entry as
                        #  it's downloaded, instead of waiting for entire
                        #  playlist to download, so we can kill yt-dlp once we
                        #  have necessary info and not have to wait (this won't
                        #  work with --dump-single-json)

                     '--dump-json',
                        # dump video data to stdout in JSON, but unlike
                        #  --dump-single-json which outputs single JSON
                        #  structure for entire playlist and includes metadata
                        #  about playlist itself, this outputs multiple JSON
                        #  newline-delimited structures (one per playlist
                        #  entry) and some of that playlist metadata exposed as
                        #  properties in each playlist entry's JSON structure

                     '--',
                     playlist_url]
    # yt-dlp supports querying for only videos uploaded after a specified date,
    #  but this feature has a few critical flaws preventing it from being used
    #  here.  For channels' or users' "/videos", "/streams", or "/shorts"
    #  pages, it cannot get exact upload dates for videos when using
    #  --flat-playlist, it can only give approximate upload dates, and only if
    #  you also pass: --extractor-args youtubetab:approximate_date
    #  The approximations are calculated from text like "1 day ago", "3 months
    #  ago", "2 years ago", etc.  As you can see the precision drops
    #  dramatically as the video gets older.  For playlist ID URLs (i.e.
    #  https://.../playlist?list=...) it can't get upload dates at all, not
    #  even approximate ones.  Even when dealing with channels/users that
    #  update frequently and have done so recently, there is some instability
    #  in the upload date reported by yt-dlp, leading to unreliable reporting.
    # Playlist maintainers (and YouTube itself) can also remove or hide videos,
    #  and later restore them, and this can also subvert the expectations of
    #  this script.  yt-dlp may also unexpectedly be missing certain videos in
    #  its output, listing them sometimes but inexplicably omitting them other
    #  times -- for example this happened when YouTube was transitioning from
    #  having all content (videos, live streams, and shorts) listed together on
    #  the "/videos" page to having them on separate pages.
    # So the most reliable technique is in fact to not filter with yt-dlp but
    #  instead record the full list of IDs it returns, and on every subsequent
    #  run merge any new IDs into that list but never remove any, which is what
    #  we do below.

    # init additional subprocess arguments
    subprocess_kwargs = {}
    if platform.system() == 'Windows':
        # On Windows, CTRL_BREAK_EVENT signal (and possibly CTRL_C_EVENT
        #  signal but the Python documentation is inconsistent with Windows
        #  documentation, see https://github.com/python/cpython/issues/57577,
        #  and we don't want to use CTRL_C_EVENT anyway because it has other
        #  issues like possibly killing this process as well) can only be sent
        #  to a process group, not an individual process, so in order to be
        #  able to send it later to this subprocess it must be in its own
        #  process group (the ID of which will be the same as the PID).
        # Also on Windows, os.kill() and sending SIGTERM are aliases for
        #  os.terminate(), which can be used on individual processes regardless
        #  of their group or this flag, but which just calls the Win32 API
        #  function TerminateProcess(), which immediately kills the process (it
        #  is not handleable), and it's better that the process have a chance
        #  to exit gracefully.
        subprocess_kwargs['creationflags'] \
                = subprocess.CREATE_NEW_PROCESS_GROUP

    # start playlist download subprocess, but don't yet wait for it to complete
    # NOTE: This calls coroutine, which returns coroutine object that we
    #        "await", which adds it to event loop (coroutine starts running)
    #        and waits for it to "complete" (creates and starts subprocess,
    #        returns Process object immediately for caller to interact with
    #        running subprocess, does not wait for subprocess to complete).
    subproc = await asyncio.create_subprocess_exec(
                            *cmd,
                            stdin=asyncio.subprocess.DEVNULL,
                            stdout=asyncio.subprocess.PIPE, # capture stdout
                            stderr=asyncio.subprocess.PIPE, # capture stderr
                            **subprocess_kwargs)
    logger.info(f'Downloading playlist at {playlist_display_text}'
                 ' (may take several minutes if many videos), started playlist'
                f' download subprocess (yt-dlp PID {subproc.pid}).')

    # init structure to hold videos listed by subprocess, and define callback
    #  function to handle receiving info about each listed video
    videos_older_to_newer = collections.deque()
    def handle_video(video):
        nonlocal playlist_name
        if playlist_name is None:
            # this playlist property is exposed as a field in each playlist
            #  entry's data object (see comment about --dump-json)
            playlist_name = video['playlist_title']
        playlist_report_data['playlist_name'] = playlist_name
        if video['id'] in already_known_video_ids:
            logger.info(f'Encountered already-known video ID {video["id"]} in'
                         ' playlist, attempting to abort playlist download'
                        f' subprocess early (yt-dlp PID {subproc.pid}).')
            if platform.system() == 'Windows':
                # See above comments for why we use CTRL_BREAK_EVENT instead of
                #  other process termination techniques.
                subproc.send_signal(signal.CTRL_BREAK_EVENT)
            else:
                subproc.terminate()
            logger.info('Terminated playlist download subprocess (yt-dlp PID'
                       f' {subproc.pid}).')
            return False
        vobj = {'id':    video['id'],
                'title': video['title']}
        video_description = video.get('description', None)
        if video_description is not None:
            vobj['description'] = video_description
        video_thumbnails = video.get('thumbnails', None)
        if isinstance(video_thumbnails, list) and len(video_thumbnails) > 0:
            vobj['thumbnail'] = video_thumbnails[-1]['url']
        videos_older_to_newer.appendleft(vobj)
        return True

    # define callback function to handle logging each line printed to stderr
    def handle_stderr_line(line):
        logger.info(f'[yt-dlp({subproc.pid})] {line}')

    # in "parallel", read playlist download subprocess's stdout until end or
    #  until already-known video is encountered, and stderr until end
    # NOTE: This calls coroutine, which returns coroutine object that we
    #        "await", which adds it to event loop (coroutine starts running)
    #        and waits for it to complete (it's passed two coroutine objects as
    #        arguments (i.e. results of calling the two "streamreader_..."
    #        coroutines), which it converts to tasks and schedules to be run in
    #        "parallel", then waits for the return values of, which it then
    #        returns to us here (in same order as coroutines were passed), of
    #        which we're only concerned with the first).
    subprocess_not_aborted \
            = (await asyncio.gather(streamreader_to_json_objs(
                                            subproc.stdout,
                                            handle_video),
                                    streamreader_to_lines(
                                            subproc.stderr,
                                            handle_stderr_line)))[0]

    logger.info('Waiting for playlist download subprocess (yt-dlp PID'
              f' {subproc.pid})...')

    # wait for playlist download subprocess to complete (if not already)
    # NOTE: This calls coroutine, which returns coroutine object that we
    #        "await", which adds it to event loop (coroutine starts running)
    #        and waits for it to complete (i.e. waits for subprocess we started
    #        above to complete).
    await subproc.wait()

    # check subprocess's exit status (but only if we had let it run to
    #  completion; exit status is irrelevant if we had killed subprocess,
    #  because that would've meant it had printed all new data we needed and
    #  we had killed it simply to avoid waiting for data we already had)
    if subprocess_not_aborted and subproc.returncode != 0:
        logger.error(f'Playlist download subprocess (yt-dlp PID {subproc.pid})'
                      ' ended unsuccessfully (exit status'
                     f' {subproc.returncode}), see "{REPORT_FILE_NAME}" for'
                      ' more details.')
        num_videos_gathered_successfully = len(videos_older_to_newer)
        if num_videos_gathered_successfully > 1:
            addl_stmt = ('but not before it returned information about'
                        f' {num_videos_gathered_successfully} videos (which'
                         ' won\'t be recorded in state file, so they should be'
                         ' reported again once you fix cause of failure)')
        elif num_videos_gathered_successfully == 1:
            addl_stmt = ('but not before it returned information about 1 video'
                         ' (which won\'t be recorded in state file, so it'
                         ' should be reported again once you fix cause of'
                         ' failure)')
        else:
            addl_stmt = 'and it returned no information about any videos'
        playlist_report_data.setdefault('notices', set()).add(
                'The playlist download subprocess ended unsuccessfully (exit'
               f' status {subproc.returncode}), {addl_stmt}. Look within'
               f' "{LOG_FILE_NAME}" for any errors logged from the playlist'
               f' download subprocess (yt-dlp PID {subproc.pid}).')
        return playlist_name

    # subprocess had completed successfully, or was aborted after it printed
    #  all new data we needed
    logger.info('Playlist download completed successfully, constructing report'
                ' data...')
    if len(already_known_video_ids) > 0 and subprocess_not_aborted:
        playlist_report_data.setdefault('notices', set()).add(
                'None of the previously known video IDs were encountered'
                ' during this run, so one or more videos may be missing in the'
                ' list below!')
    if len(videos_older_to_newer) > 0:
        playlist_report_data.setdefault('videos_older_to_newer',
                                        []).extend(videos_older_to_newer)
    logger.info('Playlist report data constructed.')

    return playlist_name

def write_report(report_file, report_data, report_start_timestamp):
    logger = logging.getLogger('write_report')
    logger.info('Writing report...')
    print('<html>\n'
          '<head>\n'
         f'<title>YouTube Follow Report @ {report_start_timestamp.strftime("%c (%Z)")}</title>\n'
          '<script type="text/javascript">\n'
          'function revealEmbed(thumbnailId)\n'
          '{\n'
          '    var embedId = thumbnailId.replace(/^thumbnail-/, "embed-");\n'
          '    document.getElementById(thumbnailId).style.display = "none";\n'
          '    var iframe = document.getElementById(embedId);\n'
          '    iframe.style.display = "";\n'
          '    iframe.src = iframe.getAttribute("data-src");\n'
          '}\n'
          '</script>\n'
          '</head>\n'
          '<body>',
          file=report_file)
    for playlist_url, playlist_report_data in report_data.items():
        # NOTE: We use CSS `padding-left` below instead of `text-indent`
        #        because the latter only indents the first contained line and
        #        we want to indent all the lines together.
        print(f'<h2><a href="{playlist_url}">{html.escape(playlist_report_data["playlist_name"])}</a></h2>\n'
               '    <div style="padding-left: 50px;">\n'
               '    New videos, older to newer:',
              file=report_file)
        for notice in playlist_report_data.get('notices', []):
            print(f'    <p><b>WARNING:</b> {html.escape(notice)}</p>',
                  file=report_file)
        for video in playlist_report_data.get('videos_older_to_newer', []):
            # NOTE: The following HTML code for embedding a YouTube video was
            #        taken from right-clicking a video on YouTube, selecting
            #        "Copy embed code", tweaking the size, and replacing static
            #        data with variables:
            html_safe_title = html.escape(video['title'])
            if 'description' in video:
                html_safe_description = html.escape(video['description']) \
                                            .replace('\n', '&#10;')
                tooltip_html = (
                   '\n'
                  f'          title="{html_safe_description}"')
            else:
                tooltip_html = ''
            print(f'    <p><a href="https://www.youtube.com/watch?v={video["id"]}"{tooltip_html}>\n'
                  f'           {html_safe_title}</a><br/>\n'
                  f'       <img id="thumbnail-{video["id"]}"\n'
                  f'            src="{video["thumbnail"]}"\n'
                   '            style="cursor:pointer;"\n'
                   '            onclick="revealEmbed(this.id)" /><br/>\n'
                  f'       <iframe id="embed-{video["id"]}"\n'
                   '               style="display:none;"\n'
                   '               width="480"\n'
                   '               height="360"\n'
                   '               src=""\n'
                  f'               data-src="https://www.youtube.com/embed/{video["id"]}"\n'
                  f'               title="{video["title"]}"\n'
                   '               frameborder="0"\n'
                   '               allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"\n'
                   '               allowfullscreen></iframe></p>',
                  file=report_file)
        print('    </div>', file=report_file)
    if len(report_data) == 0:
        print('No new videos.', file=report_file)
    print('</body>\n'
          '</html>',
          file=report_file)
    logger.info('Report written.')

def use_report(report_file_name,
               *,
               launch=False,
               prompt_to_delete=False,
               delete=None):
    logger = logging.getLogger('use_report')
    if delete is None:
        delete = prompt_to_delete
    elif prompt_to_delete and not delete:
        raise ValueError('cannot have prompt_to_delete be True while delete is'
                         ' False')
    if not os.path.exists(report_file_name):
        logger.info(f'Report file "{report_file_name}" not found, nothing to'
                     ' do.')
        return
    with ClaimedFile(lock_file_name=f'{report_file_name}.lck'):
        if launch:
            logger.info(f'Launching report file "{report_file_name}" in web'
                         ' browser.')
            file_url = pathlib.Path(report_file_name).absolute().as_uri()

            # webbrowser module may produce output, and provides no direct
            #  means of suppressing it, so we just redirect stdout+stderr to
            #  /dev/null (or Windows' equivalent) for duration of open_new_tab
            with RedirectStdIO(sys.stdout, os.devnull), \
                 RedirectStdIO(sys.stderr, os.devnull):
                webbrowser.open_new_tab(file_url)

        if prompt_to_delete:
            if not confirmation_prompt(
                           f'Delete report file "{report_file_name}"? ',
                           negative_feedback=('Response not recognized, must'
                                              ' be "y" or "n".')):
                return
        if delete:
            logger.info(f'Deleting report file "{report_file_name}". Videos'
                         ' listed in it will not be reported again.')
            os.remove(report_file_name)
        else:
            logger.info(f'Keeping report file "{report_file_name}". If it is'
                         ' still present next time this is run in "generate"'
                         ' mode, it will be replaced with a report file'
                         ' listing the same videos plus any newer ones.')

def main__usage():
    print(USAGE.format(program=os.path.basename(sys.argv[0])))
    return 1

# NOTE: When this function is called, it just returns a coroutine object, it
#        doesn't run the code you see below.
async def main__generate_report__async():
    # init
    program_start_timestamp_utc = datetime.datetime.now(datetime.timezone.utc)

    # setup logging
    try:
        os.remove(LOG_FILE_NAME)
    except FileNotFoundError:
        pass
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    fh = logging.FileHandler(LOG_FILE_NAME, mode='x', encoding='UTF-8')
    fh.setLevel(logging.DEBUG)
    class UTCFormatter(logging.Formatter):
        converter = time.gmtime
    fmtr = UTCFormatter('[%(asctime)s.%(msecs)03dZ]'
                        ' %(levelname)s: %(name)s: %(message)s',
                        datefmt='%Y-%m-%dT%H:%M:%S')
    fh.setFormatter(fmtr)
    root_logger.addHandler(fh)
    print(f'See "{LOG_FILE_NAME}" to monitor progress.')
    logger = logging.getLogger('main__generate_report__async')
    logger.info(f'Program started as PID {os.getpid()} at'
                f' {program_start_timestamp_utc}, using config file'
                f' "{CONFIG_FILE_NAME}", state file "{STATE_FILE_NAME}", and'
                f' report file "{REPORT_FILE_NAME}".')

    # setup signal handling for Windows
    if platform.system() == 'Windows':
        windows_setup_graceful_exit_for_python_unhandled_control_signals()

    # init/read files
    with ClaimedFile(WriteProxiedFile(STATE_FILE_NAME, encoding='UTF-8')) \
            as (state_file, state_write_proxy_file):
        with ClaimedFile(WriteProxiedFile(CONFIG_FILE_NAME,
                                          must_already_exist=True,
                                          num_backups=float('inf'),
                                          encoding='UTF-8')) \
                as (config_file, config_write_proxy_file):
            config_modified = False
            config = read_config(config_file)
            with ClaimedFile(WriteProxiedFile(REPORT_FILE_NAME,
                                              encoding='UTF-8')) \
                    as (report_file, report_write_proxy_file):
                (video_ids_acked_before_last_report,
                 last_report_data) = read_state(state_file)
                # NOTE: Later, when writing new state, we'll compare it to the
                #        state we just read, so in both cases below we must
                #        deep copy anything which will/may get modified.
                if report_file is None:
                    # No report file exists, which should mean either:
                    #   * This program is running for the first time (in the
                    #      current directory), which means there's also no
                    #      state file yet, so the state object we have now is
                    #      empty.
                    #   * The user already viewed and deleted it, so we need to
                    #      create a new one using the state as it was after
                    #      that deleted report was generated (which we'll
                    #      construct by adding the IDs in last report's data to
                    #      the IDs that the user had acknowledged before the
                    #      last report was generated).
                    logger.debug('Report file not found, so set of user-acked'
                                 ' video IDs = set of video IDs acked before'
                                 ' last report (if any) + set of video IDs'
                                 ' used in last report (if any), and new'
                                 ' report\'s data starts out empty.')
                    acked_video_ids = (
                            {} if video_ids_acked_before_last_report is None
                            else copy.deepcopy(
                                         video_ids_acked_before_last_report))
                    if last_report_data is not None:
                        for (playlist_url,
                             playlist_report_data) in last_report_data.items():
                            acked_video_ids.setdefault(playlist_url,
                                                       set()).update(
                                    {v['id']
                                     for v
                                      in playlist_report_data[
                                                 'videos_older_to_newer']})
                    new_report_data = {}
                else:
                    # A report file already exists, so we need to replace it
                    #  with a fresh one that includes everything currently in
                    #  it, plus anything new since.
                    logger.debug('Report file found, so set of user-acked'
                                 ' video IDs = set of video IDs acked before'
                                 ' last report (if any), and new report\'s'
                                 ' data starts out with last report\'s data.')
                    acked_video_ids = (
                            {} if video_ids_acked_before_last_report is None
                            else video_ids_acked_before_last_report)
                    new_report_data = (
                            {} if last_report_data is None
                            else copy.deepcopy(last_report_data))

                # gather info
                for config__playlist_info in config['playlist_infos']:
                    playlist_url = config__playlist_info['url']
                    playlist_acked_video_ids \
                            = acked_video_ids.get(playlist_url, set())
                    playlist_new_report_data \
                            = new_report_data.setdefault(playlist_url, {})
                    playlist_name = config__playlist_info.get('name', None)

                    # get playlist data
                    # NOTE: This calls coroutine, which returns coroutine
                    #        object that we "await", which adds it to event
                    #        loop (coroutine starts running) and waits for it
                    #        to complete.
                    await get_playlist_report_data__async(
                                  playlist_new_report_data,
                                  playlist_acked_video_ids,
                                  playlist_url,
                                  playlist_name)

                    if (    playlist_name is None
                        and 'playlist_name' in playlist_new_report_data):
                        playlist_name \
                                = config__playlist_info['name'] \
                                = playlist_new_report_data['playlist_name']
                        config_modified = True
                    if not any(field in playlist_new_report_data
                               for field in ('notices',
                                             'videos_older_to_newer')):
                        # NOTE: There are no notices or videos in this
                        #        playlist's report data, so we must delete its
                        #        entry (which we must've just recently created
                        #        when we called `.setdefault(...)` on
                        #        `new_report_data` above) because the rest of
                        #        the code assumes that we only have entries for
                        #        playlists that actually have data to go into
                        #        the report (i.e. either notices or videos).
                        del playlist_new_report_data
                        del new_report_data[playlist_url]

                # write report
                write_report(report_write_proxy_file,
                             new_report_data,
                             program_start_timestamp_utc)
                # NOTE: If write_report or closing of report file's `with`
                #        block fails for any reason, exception is raised before
                #        we write actual report file, so we'd also skip logic
                #        below for updating playlist names in config file (no
                #        big deal) and writing updated state file, thus in the
                #        end none of the files get written and it's as if
                #        program just never ran, which is exactly the
                #        transactional behavior we want.
                #       If report existed at start, but was removed by time of
                #        writing (for instance because user had opened report
                #        without using this program's "view" mode, watched
                #        videos, then deleted report, all while this program
                #        was running) then newly written report will contain
                #        videos user already saw (because report had existed at
                #        the time this program started). While this may be
                #        annoying, it is at least better than alternative of
                #        erring in favor of skipping videos user may not have
                #        seen. To prevent this from happening, user should only
                #        view/delete report via this program's "view" mode.

            # write config, if it's been modified
            if not config_modified:
                raise AbortProxyWrite
            write_config(config_write_proxy_file, config)
            # NOTE: If write_config or closing of config file's `with` block
            #        fails for any reason, it's not a big deal because new
            #        config (which failed to write) just contained some names
            #        of playlists that had names unset in original config (not
            #        a big deal).

        # write state, if it's been modified
        if (    (acked_video_ids or None) == video_ids_acked_before_last_report
            and (new_report_data or None) == last_report_data):
            raise AbortProxyWrite
        write_state(state_write_proxy_file, acked_video_ids, new_report_data)
        # NOTE: If write_state or closing of state file's `with` block fails
        #        for any reason, it's not a big deal because previous state
        #        file would still exist, and worst case user will be
        #        re-notified of some already-seen videos on next run of
        #        program. If we had written state *before* writing report, a
        #        failed report write (after successful state write) would mean
        #        we might actually skip reporting some videos, which we
        #        definitely don't want to happen.

def main__generate_report():
    # NOTE: This calls coroutine, which returns coroutine object, which is
    #        passed to .run, which creates an event loop and adds coroutine to
    #        it (coroutine starts running) and waits for it to complete.
    asyncio.run(main__generate_report__async())

def main__view_report():
    # setup logging
    logging.basicConfig(stream=sys.stdout,
                        format='%(levelname)s: %(name)s: %(message)s',
                        level=logging.INFO)
    logger = logging.getLogger('main__view_report')
    logger.info(f'Program started as PID {os.getpid()},'
                f' using report file "{REPORT_FILE_NAME}".')

    # setup signal handling for Windows
    if platform.system() == 'Windows':
        windows_setup_graceful_exit_for_python_unhandled_control_signals()

    # open report
    use_report(REPORT_FILE_NAME, launch=True, prompt_to_delete=True)

def main():
    args = sys.argv[1:]
    try:
        if len(args) != 1:
            return main__usage()
        return {
            'generate': main__generate_report,
            'view':     main__view_report
        }.get(args[0], main__usage)()
    except KeyboardInterrupt:
        print('\nUser interrupted program, will now exit.', file=sys.stderr)
        return 1

if __name__ == '__main__':
    sys.exit(main())
